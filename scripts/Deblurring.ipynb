{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "collapsed_sections": [
        "dv7S1Fj0k3c2",
        "jcBWem2QII6y",
        "AfBfTMegZWFj"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Deblurring**"
      ],
      "metadata": {
        "id": "dv7S1Fj0k3c2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from pdb import set_trace as stx\n",
        "\n",
        "\n",
        "def conv(in_channels, out_channels, kernel_size, bias=False, stride=1):\n",
        "    return nn.Conv2d(\n",
        "        in_channels, out_channels, kernel_size,\n",
        "        padding=(kernel_size // 2), bias=bias, stride=stride)\n",
        "\n",
        "\n",
        "def conv_down(in_chn, out_chn, bias=False):\n",
        "    layer = nn.Conv2d(in_chn, out_chn, kernel_size=4, stride=2, padding=1, bias=bias)\n",
        "    return layer\n",
        "\n",
        "\n",
        "def default_conv(in_channels, out_channels, kernel_size,stride=1, bias=True):\n",
        "    return nn.Conv2d(\n",
        "        in_channels, out_channels, kernel_size,\n",
        "        padding=(kernel_size//2),stride=stride, bias=bias)\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self, conv, n_feats, kernel_size,\n",
        "        bias=True, bn=False, act=nn.PReLU(), res_scale=1):\n",
        "\n",
        "        super(ResBlock, self).__init__()\n",
        "        m = []\n",
        "        for i in range(2):\n",
        "            if i == 0:\n",
        "                m.append(conv(n_feats, 64, kernel_size, bias=bias))\n",
        "            else:\n",
        "                m.append(conv(64, n_feats, kernel_size, bias=bias))\n",
        "            if bn:\n",
        "                m.append(nn.BatchNorm2d(n_feats))\n",
        "            if i == 0:\n",
        "                m.append(act)\n",
        "\n",
        "        self.body = nn.Sequential(*m)\n",
        "        self.res_scale = res_scale\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = self.body(x).mul(self.res_scale)\n",
        "        res += x\n",
        "\n",
        "        return res\n",
        "\n",
        "\n",
        "class CAB(nn.Module):\n",
        "    def __init__(self, n_feat, kernel_size, reduction, bias, act):\n",
        "        super(CAB, self).__init__()\n",
        "        modules_body = []\n",
        "        modules_body.append(conv(n_feat, n_feat, kernel_size, bias=bias))\n",
        "        modules_body.append(act)\n",
        "        modules_body.append(conv(n_feat, n_feat, kernel_size, bias=bias))\n",
        "\n",
        "        self.CA = CALayer(n_feat, reduction, bias=bias)\n",
        "        self.body = nn.Sequential(*modules_body)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = self.body(x)\n",
        "        res = self.CA(res)\n",
        "        res += x\n",
        "        return res\n",
        "\n",
        "\n",
        "class CALayer(nn.Module):\n",
        "    def __init__(self, channel, reduction=16, bias=False):\n",
        "        super(CALayer, self).__init__()\n",
        "        # global average pooling: feature --> point\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        # feature channel downscale and upscale --> channel weight\n",
        "        self.conv_du = nn.Sequential(\n",
        "            nn.Conv2d(channel, channel // reduction, 1, padding=0, bias=bias),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channel // reduction, channel, 1, padding=0, bias=bias),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.avg_pool(x)\n",
        "        y = self.conv_du(y)\n",
        "        return x * y\n",
        "\n",
        "\n",
        "class SAM(nn.Module):\n",
        "    def __init__(self, n_feat, kernel_size, bias):\n",
        "        super(SAM, self).__init__()\n",
        "        self.conv1 = conv(n_feat, n_feat, kernel_size, bias=bias)\n",
        "        self.conv2 = conv(n_feat, 3, kernel_size, bias=bias)\n",
        "\n",
        "    def forward(self, x, x_img):\n",
        "        x1 = self.conv1(x)\n",
        "        img = self.conv2(x) + x_img\n",
        "        x1 = x1 + x\n",
        "        return x1, img\n",
        "\n",
        "\n",
        "class mergeblock(nn.Module):\n",
        "    def __init__(self, n_feat, kernel_size, bias, subspace_dim=16):\n",
        "        super(mergeblock, self).__init__()\n",
        "        self.conv_block = conv(n_feat * 2, n_feat, kernel_size, bias=bias)\n",
        "        self.num_subspace = subspace_dim\n",
        "        self.subnet = conv(n_feat * 2, self.num_subspace, kernel_size, bias=bias)\n",
        "\n",
        "    def forward(self, x, bridge):\n",
        "        out = torch.cat([x, bridge], 1)\n",
        "        b_, c_, h_, w_ = bridge.shape\n",
        "        sub = self.subnet(out)\n",
        "        V_t = sub.view(b_, self.num_subspace, h_*w_)\n",
        "        V_t = V_t / (1e-6 + torch.abs(V_t).sum(axis=2, keepdims=True))\n",
        "        V = V_t.permute(0, 2, 1)\n",
        "        mat = torch.matmul(V_t, V)\n",
        "        mat_inv = torch.inverse(mat)\n",
        "        project_mat = torch.matmul(mat_inv, V_t)\n",
        "        bridge_ = bridge.view(b_, c_, h_*w_)\n",
        "        project_feature = torch.matmul(project_mat, bridge_.permute(0, 2, 1))\n",
        "        bridge = torch.matmul(V, project_feature).permute(0, 2, 1).view(b_, c_, h_, w_)\n",
        "        out = torch.cat([x, bridge], 1)\n",
        "        out = self.conv_block(out)\n",
        "        return out+x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, n_feat, kernel_size, reduction, act, bias, scale_unetfeats, csff,depth=5):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.body=nn.ModuleList()#[]\n",
        "        self.depth=depth\n",
        "        for i in range(depth-1):\n",
        "            self.body.append(UNetConvBlock(in_size=n_feat+scale_unetfeats*i, out_size=n_feat+scale_unetfeats*(i+1), downsample=True, relu_slope=0.2, use_csff=csff, use_HIN=True))\n",
        "        self.body.append(UNetConvBlock(in_size=n_feat+scale_unetfeats*(depth-1), out_size=n_feat+scale_unetfeats*(depth-1), downsample=False, relu_slope=0.2, use_csff=csff, use_HIN=True))\n",
        "\n",
        "    def forward(self, x, encoder_outs=None, decoder_outs=None):\n",
        "        res=[]\n",
        "        if encoder_outs is not None and decoder_outs is not None:\n",
        "            for i,down in enumerate(self.body):\n",
        "                if (i+1) < self.depth:\n",
        "                    x, x_up = down(x,encoder_outs[i],decoder_outs[-i-1])\n",
        "                    res.append(x_up)\n",
        "                else:\n",
        "                    x = down(x)\n",
        "        else:\n",
        "            for i,down in enumerate(self.body):\n",
        "                if (i+1) < self.depth:\n",
        "                    x, x_up = down(x)\n",
        "                    res.append(x_up)\n",
        "                else:\n",
        "                    x = down(x)\n",
        "        return res,x\n",
        "\n",
        "\n",
        "class UNetConvBlock(nn.Module):\n",
        "    def __init__(self, in_size, out_size, downsample, relu_slope, use_csff=False, use_HIN=False):\n",
        "        super(UNetConvBlock, self).__init__()\n",
        "        self.downsample = downsample\n",
        "        self.identity = nn.Conv2d(in_size, out_size, 1, 1, 0)\n",
        "        self.use_csff = use_csff\n",
        "\n",
        "        self.conv_1 = nn.Conv2d(in_size, out_size, kernel_size=3, padding=1, bias=True)\n",
        "        self.relu_1 = nn.LeakyReLU(relu_slope, inplace=False)\n",
        "        self.conv_2 = nn.Conv2d(out_size, out_size, kernel_size=3, padding=1, bias=True)\n",
        "        self.relu_2 = nn.LeakyReLU(relu_slope, inplace=False)\n",
        "\n",
        "        if downsample and use_csff:\n",
        "            self.csff_enc = nn.Conv2d(out_size, out_size, 3, 1, 1)\n",
        "            self.csff_dec = nn.Conv2d(in_size, out_size, 3, 1, 1)\n",
        "            self.phi = nn.Conv2d(out_size, out_size, 3, 1, 1)\n",
        "            self.gamma = nn.Conv2d(out_size, out_size, 3, 1, 1)\n",
        "\n",
        "        if use_HIN:\n",
        "            self.norm = nn.InstanceNorm2d(out_size//2, affine=True)\n",
        "        self.use_HIN = use_HIN\n",
        "\n",
        "        if downsample:\n",
        "            self.downsample = conv_down(out_size, out_size, bias=False)\n",
        "\n",
        "    def forward(self, x, enc=None, dec=None):\n",
        "        out = self.conv_1(x)\n",
        "\n",
        "        if self.use_HIN:\n",
        "            out_1, out_2 = torch.chunk(out, 2, dim=1)\n",
        "            out = torch.cat([self.norm(out_1), out_2], dim=1)\n",
        "        out = self.relu_1(out)\n",
        "        out = self.relu_2(self.conv_2(out))\n",
        "\n",
        "        out += self.identity(x)\n",
        "        if enc is not None and dec is not None:\n",
        "            assert self.use_csff\n",
        "            skip_ = F.leaky_relu(self.csff_enc(enc) + self.csff_dec(dec), 0.1, inplace=True)\n",
        "            out = out*F.sigmoid(self.phi(skip_)) + self.gamma(skip_) + out\n",
        "        if self.downsample:\n",
        "            out_down = self.downsample(out)\n",
        "            return out_down, out\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "\n",
        "class UNetUpBlock(nn.Module):\n",
        "    def __init__(self, in_size, out_size, relu_slope):\n",
        "        super(UNetUpBlock, self).__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2, bias=True)\n",
        "        self.conv_block = UNetConvBlock(out_size*2, out_size, False, relu_slope)\n",
        "\n",
        "    def forward(self, x, bridge):\n",
        "        up = self.up(x)\n",
        "        out = torch.cat([up, bridge], 1)\n",
        "        out = self.conv_block(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, n_feat, kernel_size, reduction, act, bias, scale_unetfeats,depth=5):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.body=nn.ModuleList()\n",
        "        self.skip_conv=nn.ModuleList()#[]\n",
        "        for i in range(depth-1):\n",
        "            self.body.append(UNetUpBlock(in_size=n_feat+scale_unetfeats*(depth-i-1), out_size=n_feat+scale_unetfeats*(depth-i-2), relu_slope=0.2))\n",
        "            self.skip_conv.append(nn.Conv2d(n_feat+scale_unetfeats*(depth-i-1), n_feat+scale_unetfeats*(depth-i-2), 3, 1, 1))\n",
        "\n",
        "    def forward(self, x, bridges):\n",
        "        res=[]\n",
        "        for i,up in enumerate(self.body):\n",
        "            x=up(x,self.skip_conv[i](bridges[-i-1]))\n",
        "            res.append(x)\n",
        "\n",
        "        return res\n",
        "\n",
        "\n",
        "class DownSample(nn.Module):\n",
        "    def __init__(self, in_channels, s_factor):\n",
        "        super(DownSample, self).__init__()\n",
        "        self.down = nn.Sequential(nn.Upsample(scale_factor=0.5, mode='bilinear', align_corners=False),\n",
        "                                  nn.Conv2d(in_channels, in_channels + s_factor, 1, stride=1, padding=0, bias=False))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.down(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UpSample(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(UpSample, self).__init__()\n",
        "        self.up = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "                                nn.Conv2d(in_channels, out_channels, 1, stride=1, padding=0, bias=False))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.up(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Basic_block(nn.Module):\n",
        "    def __init__(self, in_c=3, out_c=3, n_feat=80, scale_unetfeats=48, scale_orsnetfeats=32, num_cab=8, kernel_size=3, reduction=4, bias=False):\n",
        "        super(Basic_block, self).__init__()\n",
        "        act = nn.PReLU()\n",
        "        self.phi_1 = ResBlock(default_conv,3,3)\n",
        "        self.phit_1 = ResBlock(default_conv,3,3)\n",
        "        self.shallow_feat2 = nn.Sequential(conv(3, n_feat, kernel_size, bias=bias),\n",
        "                                           CAB(n_feat, kernel_size, reduction, bias=bias, act=act))\n",
        "        self.stage2_encoder = Encoder(n_feat, kernel_size, reduction, act, bias, scale_unetfeats,depth=4, csff=True)\n",
        "        self.stage2_decoder = Decoder(n_feat, kernel_size, reduction, act, bias, scale_unetfeats,depth=4)\n",
        "        self.sam23 = SAM(n_feat, kernel_size=1, bias=bias)\n",
        "        self.r1 = nn.Parameter(torch.Tensor([0.5]))\n",
        "        self.concat12 = conv(n_feat * 2, n_feat, kernel_size, bias=bias)\n",
        "\n",
        "        self.merge12=mergeblock(n_feat,3,True)\n",
        "\n",
        "    def forward(self, img,stage1_img,feat1,res1,x2_samfeats):\n",
        "        phixsy_2 = self.phi_1(stage1_img) - img\n",
        "        x2_img = stage1_img - self.r1*self.phit_1(phixsy_2)\n",
        "        x2 = self.shallow_feat2(x2_img)\n",
        "        x2_cat = self.merge12(x2, x2_samfeats)\n",
        "        feat2,feat_fin2 = self.stage2_encoder(x2_cat, feat1, res1)\n",
        "        res2 = self.stage2_decoder(feat_fin2,feat2)\n",
        "        x3_samfeats, stage2_img = self.sam23(res2[-1], x2_img)\n",
        "        return x3_samfeats, stage2_img, feat2, res2\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_c=3, out_c=3, n_feat=80, scale_unetfeats=48, scale_orsnetfeats=32, num_cab=8, kernel_size=3,\n",
        "                 reduction=4, bias=False, depth=5):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        act = nn.PReLU()\n",
        "        self.depth=depth\n",
        "        self.basic=Basic_block(in_c, out_c, n_feat, scale_unetfeats, scale_orsnetfeats, num_cab, kernel_size, reduction, bias)\n",
        "        self.shallow_feat1 = nn.Sequential(conv(3, n_feat, kernel_size, bias=bias),\n",
        "                                           CAB(n_feat, kernel_size, reduction, bias=bias, act=act))\n",
        "        self.shallow_feat7 = nn.Sequential(conv(3, n_feat, kernel_size, bias=bias),\n",
        "                                           CAB(n_feat, kernel_size, reduction, bias=bias, act=act))\n",
        "\n",
        "        self.stage1_encoder = Encoder(n_feat, kernel_size, reduction, act, bias, scale_unetfeats,depth=4, csff=False)\n",
        "        self.stage1_decoder = Decoder(n_feat, kernel_size, reduction, act, bias, scale_unetfeats,depth=4)\n",
        "\n",
        "        self.sam12 = SAM(n_feat, kernel_size=1, bias=bias)\n",
        "\n",
        "        self.phi_0 = ResBlock(default_conv,3,3)\n",
        "        self.phit_0 = ResBlock(default_conv,3,3)\n",
        "        self.phi_6 = ResBlock(default_conv,3,3)\n",
        "        self.phit_6 = ResBlock(default_conv,3,3)\n",
        "        self.r0 = nn.Parameter(torch.Tensor([0.5]))\n",
        "        self.r6 = nn.Parameter(torch.Tensor([0.5]))\n",
        "\n",
        "        self.concat67 = conv(n_feat * 2, n_feat + scale_orsnetfeats, kernel_size, bias=bias)\n",
        "        self.tail = conv(n_feat + scale_orsnetfeats, 3, kernel_size, bias=bias)\n",
        "\n",
        "    def forward(self, img):\n",
        "        res=[]\n",
        "        phixsy_1 = self.phi_0(img) - img\n",
        "        x1_img = img - self.r0*self.phit_0(phixsy_1)\n",
        "        x1 = self.shallow_feat1(x1_img)\n",
        "        feat1,feat_fin1 = self.stage1_encoder(x1)\n",
        "        res1 = self.stage1_decoder(feat_fin1,feat1)\n",
        "        x2_samfeats, stage1_img = self.sam12(res1[-1], x1_img)\n",
        "        res.append(stage1_img)\n",
        "\n",
        "        for _ in range(self.depth):\n",
        "            x2_samfeats, stage1_img, feat1, res1 = self.basic(img,stage1_img,feat1,res1,x2_samfeats)\n",
        "            res.append(stage1_img)\n",
        "        phixsy_7 = self.phi_6(stage1_img) - img\n",
        "        x7_img = stage1_img - self.r6*self.phit_6(phixsy_7)\n",
        "        x7 = self.shallow_feat7(x7_img)\n",
        "        x7_cat = self.concat67(torch.cat([x7, x2_samfeats], 1))\n",
        "        stage7_img = self.tail(x7_cat)+ img\n",
        "        res.append(stage7_img)\n",
        "\n",
        "        return res[::-1]\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_c=3, n_feat=64, num_layers=5):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        layers = [\n",
        "            nn.Conv2d(in_c, n_feat, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        ]\n",
        "\n",
        "        for i in range(1, num_layers):\n",
        "            in_channels = n_feat * min(2**(i-1), 8)\n",
        "            out_channels = n_feat * min(2**i, 8)\n",
        "            layers += [\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2 if i < num_layers - 1 else 1, padding=1, bias=False),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.LeakyReLU(0.2, inplace=True)\n",
        "            ]\n",
        "\n",
        "        layers.append(nn.Conv2d(out_channels, 1, kernel_size=4, stride=1, padding=1))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "ZE2h9x5PlDF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python"
      ],
      "metadata": {
        "id": "kFtHqh9kJx2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def torchPSNR(tar_img, prd_img):\n",
        "    imdff = torch.clamp(prd_img,0,1) - torch.clamp(tar_img,0,1)\n",
        "    rmse = (imdff**2).mean().sqrt()\n",
        "    ps = 20*torch.log10(1/rmse)\n",
        "    return ps\n",
        "\n",
        "def numpyPSNR(tar_img, prd_img):\n",
        "    imdff = np.float32(prd_img) - np.float32(tar_img)\n",
        "    rmse = np.sqrt(np.mean(imdff**2))\n",
        "    ps = 20*np.log10(255/rmse)\n",
        "    return ps"
      ],
      "metadata": {
        "id": "iAcBMU0-lY8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from torchvision.transforms import functional as TF\n",
        "class GoProDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_pairs = []\n",
        "\n",
        "        for subdir in os.listdir(root_dir):\n",
        "            blur_dir = os.path.join(root_dir, subdir, 'blur_gamma')\n",
        "            if os.path.isdir(blur_dir):\n",
        "                common_images = os.listdir(blur_dir)\n",
        "                for img_name in common_images:\n",
        "                    self.image_pairs.append(os.path.join(blur_dir, img_name))\n",
        "\n",
        "        # Define a transformation to ensure all images are (256, 256, 3)\n",
        "        self.default_transform = transforms.Compose([  # Resize to 256x256\n",
        "            transforms.ToTensor(),         # Convert to tensor (C, H, W)\n",
        "            transforms.Lambda(lambda x: x[:3, :, :])  # Ensure 3 channels\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_pairs[idx]\n",
        "        blur_image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Apply transformations\n",
        "        blur_image = TF.to_tensor(blur_image)\n",
        "\n",
        "        return blur_image, img_path"
      ],
      "metadata": {
        "id": "Xug60BGGyiFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing code and score**"
      ],
      "metadata": {
        "id": "jcBWem2QII6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "from collections import OrderedDict\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from skimage import img_as_ubyte\n",
        "from pdb import set_trace as stx\n",
        "class Config:\n",
        "  input_dir='/content/drive/MyDrive/Major/ImageRestoration/deblur_input'\n",
        "  result_dir='./drive/MyDrive/Major/ImageRestoration/output_deblur'\n",
        "  weights='./drive/MyDrive/Major/ImageRestoration/m/Deblurring/Generator.pth'\n",
        "  dataset='GoPro',\n",
        "\n",
        "args = Config()\n",
        "\n",
        "model = Generator()\n",
        "checkpoint = torch.load(args.weights)\n",
        "try:\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "except:\n",
        "    state_dict = checkpoint[\"state_dict\"]\n",
        "    new_state_dict = OrderedDict()\n",
        "    for k, v in state_dict.items():\n",
        "        name = k[7:] # remove `module.`\n",
        "        new_state_dict[name] = v\n",
        "    model.load_state_dict(new_state_dict)\n",
        "print(\"===>Testing using weights: \",args.weights)\n",
        "model.cuda()\n",
        "model = nn.DataParallel(model)\n",
        "model.eval()\n",
        "dataset = args.dataset\n",
        "test_dataset=GoProDataset(root_dir=args.input_dir)\n",
        "result_dir=args.result_dir\n",
        "test_loader  = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False, num_workers=4, drop_last=False, pin_memory=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, data_test in enumerate(tqdm(test_loader), 0):\n",
        "        torch.cuda.ipc_collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        input_    = data_test[0].cuda()\n",
        "        filenames = data_test[1]\n",
        "\n",
        "        # Padding in case images are not multiples of 8\n",
        "        restored = model(input_)\n",
        "        restored = torch.clamp(restored[0],0,1)\n",
        "\n",
        "        # Unpad images to original dimensions\n",
        "        print(filenames)\n",
        "        restored = restored.permute(0, 2, 3, 1).cpu().detach().numpy()\n",
        "        restored_im = img_as_ubyte(restored[0])\n",
        "        cv2.imwrite(result_dir+\"/\"+str(filenames[0].split(\"/\")[-1]),cv2.cvtColor(restored_im, cv2.COLOR_RGB2BGR))\n",
        "        print(result_dir+\"/\"+str(filenames[0].split(\"/\")[-1]))"
      ],
      "metadata": {
        "id": "5tgBLppo5aHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#score evaluation\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "datasets=[\"GOPRO\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    file_path = os.path.join(\"/content/drive/MyDrive/ImageRestoration/GOPRO_Large/test/GOPR0868_11_00/sharp\")\n",
        "    gt_path = os.path.join(\"/content/drive/MyDrive/ImageRestoration/output/out\")\n",
        "\n",
        "    image_files = [f for f in os.listdir(file_path) if f.endswith(('.jpg', '.png'))]\n",
        "    gt_files = [f for f in os.listdir(gt_path) if f.endswith(('.jpg', '.png'))]\n",
        "    image_pairs= []\n",
        "    common_images = set(os.listdir(file_path)).intersection(os.listdir(gt_path))\n",
        "    for img_name in common_images:\n",
        "      image_pairs.append((os.path.join(file_path, img_name), os.path.join(gt_path, img_name)))\n",
        "    total_psnr = 0\n",
        "    total_ssim = 0\n",
        "    img_num = len(image_files)\n",
        "\n",
        "    if img_num > 0:\n",
        "        for image_name, gt_name in image_pairs:\n",
        "            input_image = cv2.imread(os.path.join(file_path, image_name))\n",
        "            gt_image = cv2.imread(os.path.join(gt_path, gt_name))\n",
        "\n",
        "            input_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2GRAY)\n",
        "            gt_image = cv2.cvtColor(gt_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            ssim_val = ssim(input_image, gt_image, data_range=gt_image.max() - gt_image.min())\n",
        "            psnr_val = psnr(input_image, gt_image, data_range=gt_image.max() - gt_image.min())\n",
        "\n",
        "            total_ssim += ssim_val\n",
        "            total_psnr += psnr_val\n",
        "\n",
        "    qm_psnr = total_psnr / img_num if img_num > 0 else 0\n",
        "    qm_ssim = total_ssim / img_num if img_num > 0 else 0\n",
        "\n",
        "    print(f'For {dataset} dataset PSNR: {qm_psnr:.4f} SSIM: {qm_ssim:.4f}')"
      ],
      "metadata": {
        "id": "1SZMhnJz35Oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training COde**"
      ],
      "metadata": {
        "id": "AfBfTMegZWFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GoProDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_pairs = []\n",
        "\n",
        "        for subdir in os.listdir(root_dir):\n",
        "            blur_dir = os.path.join(root_dir, subdir, 'blur')\n",
        "            sharp_dir = os.path.join(root_dir, subdir, 'sharp')\n",
        "\n",
        "            if os.path.isdir(blur_dir) and os.path.isdir(sharp_dir):\n",
        "                common_images = set(os.listdir(blur_dir)).intersection(os.listdir(sharp_dir))\n",
        "                for img_name in common_images:\n",
        "                    self.image_pairs.append((os.path.join(blur_dir, img_name), os.path.join(sharp_dir, img_name)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        blur_image = Image.open(self.image_pairs[idx][0]).convert('RGB')\n",
        "        sharp_image = Image.open(self.image_pairs[idx][1]).convert('RGB')\n",
        "        blur_image = TF.to_tensor(blur_image)\n",
        "        sharp_image = TF.to_tensor(sharp_image)\n",
        "        if self.transform:\n",
        "            blur_image = self.transform(blur_image)\n",
        "            sharp_image = self.transform(sharp_image)\n",
        "        return blur_image, sharp_image"
      ],
      "metadata": {
        "id": "isMvKQPkZYc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def get_training_data(rgb_dir, img_options):\n",
        "    assert os.path.exists(rgb_dir)\n",
        "    return DataLoaderTrain(rgb_dir, img_options)\n",
        "\n",
        "def get_training_data_all(rgb_dir, img_options):\n",
        "    for dir in rgb_dir:\n",
        "        assert os.path.exists(dir)\n",
        "    return DataLoaderTrain_all(rgb_dir, img_options)\n",
        "\n",
        "def get_validation_data(rgb_dir, img_options):\n",
        "    assert os.path.exists(rgb_dir)\n",
        "    return DataLoaderVal(rgb_dir, img_options)\n",
        "\n",
        "def get_test_data(rgb_dir, img_options):\n",
        "    assert os.path.exists(rgb_dir)\n",
        "    return DataLoaderTest(rgb_dir, img_options)"
      ],
      "metadata": {
        "id": "S0FxFIhTZbsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "from PIL import Image\n",
        "import torchvision.transforms.functional as TF\n",
        "from pdb import set_trace as stx\n",
        "import random\n",
        "\n",
        "def is_image_file(filename):\n",
        "    return any(filename.endswith(extension) for extension in ['jpeg', 'JPEG', 'jpg', 'png', 'JPG', 'PNG', 'gif'])\n",
        "\n",
        "class DataLoaderTrain(Dataset):\n",
        "    def __init__(self, rgb_dir, img_options=None):\n",
        "        super(DataLoaderTrain, self).__init__()\n",
        "\n",
        "        inp_files = sorted(os.listdir(os.path.join(rgb_dir, 'input')))\n",
        "        tar_files = sorted(os.listdir(os.path.join(rgb_dir, 'target')))\n",
        "\n",
        "        self.inp_filenames = [os.path.join(rgb_dir, 'input', x)  for x in inp_files if is_image_file(x)]\n",
        "        self.tar_filenames = [os.path.join(rgb_dir, 'target', x) for x in tar_files if is_image_file(x)]\n",
        "\n",
        "        self.img_options = img_options\n",
        "        self.sizex       = len(self.tar_filenames)  # get the size of target\n",
        "\n",
        "        self.ps = self.img_options['patch_size']\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.sizex\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        index_ = index % self.sizex\n",
        "        ps = self.ps\n",
        "\n",
        "        inp_path = self.inp_filenames[index_]\n",
        "        tar_path = self.tar_filenames[index_]\n",
        "\n",
        "        inp_img = Image.open(inp_path)\n",
        "        tar_img = Image.open(tar_path)\n",
        "\n",
        "        w,h = tar_img.size\n",
        "        padw = ps-w if w<ps else 0\n",
        "        padh = ps-h if h<ps else 0\n",
        "\n",
        "        # Reflect Pad in case image is smaller than patch_size\n",
        "        if padw!=0 or padh!=0:\n",
        "            inp_img = TF.pad(inp_img, (0,0,padw,padh), padding_mode='reflect')\n",
        "            tar_img = TF.pad(tar_img, (0,0,padw,padh), padding_mode='reflect')\n",
        "\n",
        "        aug    = random.randint(0, 2)\n",
        "        if aug == 1:\n",
        "            inp_img = TF.adjust_gamma(inp_img, 1)\n",
        "            tar_img = TF.adjust_gamma(tar_img, 1)\n",
        "\n",
        "        aug    = random.randint(0, 2)\n",
        "        if aug == 1:\n",
        "            sat_factor = 1 + (0.2 - 0.4*np.random.rand())\n",
        "            inp_img = TF.adjust_saturation(inp_img, sat_factor)\n",
        "            tar_img = TF.adjust_saturation(tar_img, sat_factor)\n",
        "\n",
        "        inp_img = TF.to_tensor(inp_img)\n",
        "        tar_img = TF.to_tensor(tar_img)\n",
        "\n",
        "        hh, ww = tar_img.shape[1], tar_img.shape[2]\n",
        "\n",
        "        rr     = random.randint(0, hh-ps)\n",
        "        cc     = random.randint(0, ww-ps)\n",
        "        aug    = random.randint(0, 8)\n",
        "\n",
        "        # Crop patch\n",
        "        inp_img = inp_img[:, rr:rr+ps, cc:cc+ps]\n",
        "        tar_img = tar_img[:, rr:rr+ps, cc:cc+ps]\n",
        "\n",
        "        # Data Augmentations\n",
        "        if aug==1:\n",
        "            inp_img = inp_img.flip(1)\n",
        "            tar_img = tar_img.flip(1)\n",
        "        elif aug==2:\n",
        "            inp_img = inp_img.flip(2)\n",
        "            tar_img = tar_img.flip(2)\n",
        "        elif aug==3:\n",
        "            inp_img = torch.rot90(inp_img,dims=(1,2))\n",
        "            tar_img = torch.rot90(tar_img,dims=(1,2))\n",
        "        elif aug==4:\n",
        "            inp_img = torch.rot90(inp_img,dims=(1,2), k=2)\n",
        "            tar_img = torch.rot90(tar_img,dims=(1,2), k=2)\n",
        "        elif aug==5:\n",
        "            inp_img = torch.rot90(inp_img,dims=(1,2), k=3)\n",
        "            tar_img = torch.rot90(tar_img,dims=(1,2), k=3)\n",
        "        elif aug==6:\n",
        "            inp_img = torch.rot90(inp_img.flip(1),dims=(1,2))\n",
        "            tar_img = torch.rot90(tar_img.flip(1),dims=(1,2))\n",
        "        elif aug==7:\n",
        "            inp_img = torch.rot90(inp_img.flip(2),dims=(1,2))\n",
        "            tar_img = torch.rot90(tar_img.flip(2),dims=(1,2))\n",
        "\n",
        "        filename = os.path.splitext(os.path.split(tar_path)[-1])[0]\n",
        "\n",
        "        return tar_img, inp_img, filename\n",
        "\n",
        "class DataLoaderTrain_all(Dataset):\n",
        "    def __init__(self, rgb_dir, img_options=None):\n",
        "        super(DataLoaderTrain_all, self).__init__()\n",
        "\n",
        "        inp_files_noise = sorted(os.listdir(os.path.join(rgb_dir[0], 'input')))\n",
        "        tar_files_noise = sorted(os.listdir(os.path.join(rgb_dir[0], 'target')))\n",
        "        inp_files_blur = sorted(os.listdir(os.path.join(rgb_dir[1], 'input')))\n",
        "        tar_files_blur = sorted(os.listdir(os.path.join(rgb_dir[1], 'target')))\n",
        "        inp_files_rain = sorted(os.listdir(os.path.join(rgb_dir[2], 'input')))\n",
        "        tar_files_rain = sorted(os.listdir(os.path.join(rgb_dir[2], 'target')))\n",
        "\n",
        "        self.inp_filenames_noise = [os.path.join(rgb_dir[0], 'input', x)  for x in inp_files_noise if is_image_file(x)]\n",
        "        self.tar_filenames_noise = [os.path.join(rgb_dir[0], 'target', x) for x in tar_files_noise if is_image_file(x)]\n",
        "        self.inp_filenames_blur = [os.path.join(rgb_dir[1], 'input', x)  for x in inp_files_blur if is_image_file(x)]\n",
        "        self.tar_filenames_blur = [os.path.join(rgb_dir[1], 'target', x) for x in tar_files_blur if is_image_file(x)]\n",
        "        self.inp_filenames_rain = [os.path.join(rgb_dir[2], 'input', x)  for x in inp_files_rain if is_image_file(x)]\n",
        "        self.tar_filenames_rain = [os.path.join(rgb_dir[2], 'target', x) for x in tar_files_rain if is_image_file(x)]\n",
        "\n",
        "        self.img_options = img_options\n",
        "        self.sizex_noise       = len(self.tar_filenames_noise)  # get the size of target\n",
        "        self.sizex_blur       = len(self.tar_filenames_blur)\n",
        "        self.sizex_rain       = len(self.tar_filenames_rain)\n",
        "\n",
        "        self.ps_noise = self.img_options['patch_size_noise']\n",
        "        self.ps_blur = self.img_options['patch_size_blur']\n",
        "        self.ps_rain = self.img_options['patch_size_rain']\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.sizex_blur\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        id_ = np.random.randint(0,3)\n",
        "        if id_==0:\n",
        "            index_ = index % self.sizex_noise\n",
        "            ps = self.ps_noise\n",
        "            inp_path = self.inp_filenames_noise[index_]\n",
        "            tar_path = self.tar_filenames_noise[index_]\n",
        "        elif id_==1:\n",
        "            index_ = index % self.sizex_blur\n",
        "            ps = self.ps_blur\n",
        "            inp_path = self.inp_filenames_blur[index_]\n",
        "            tar_path = self.tar_filenames_blur[index_]\n",
        "        else:\n",
        "            index_ = index % self.sizex_rain\n",
        "            ps = self.ps_rain\n",
        "            inp_path = self.inp_filenames_rain[index_]\n",
        "            tar_path = self.tar_filenames_rain[index_]\n",
        "\n",
        "        inp_img = Image.open(inp_path)\n",
        "        tar_img = Image.open(tar_path)\n",
        "\n",
        "        w,h = tar_img.size\n",
        "        padw = ps-w if w<ps else 0\n",
        "        padh = ps-h if h<ps else 0\n",
        "\n",
        "        # Reflect Pad in case image is smaller than patch_size\n",
        "        if padw!=0 or padh!=0:\n",
        "            inp_img = TF.pad(inp_img, (0,0,padw,padh), padding_mode='reflect')\n",
        "            tar_img = TF.pad(tar_img, (0,0,padw,padh), padding_mode='reflect')\n",
        "\n",
        "        aug    = random.randint(0, 2)\n",
        "        if aug == 1:\n",
        "            inp_img = TF.adjust_gamma(inp_img, 1)\n",
        "            tar_img = TF.adjust_gamma(tar_img, 1)\n",
        "\n",
        "        aug    = random.randint(0, 2)\n",
        "        if aug == 1:\n",
        "            sat_factor = 1 + (0.2 - 0.4*np.random.rand())\n",
        "            inp_img = TF.adjust_saturation(inp_img, sat_factor)\n",
        "            tar_img = TF.adjust_saturation(tar_img, sat_factor)\n",
        "\n",
        "        inp_img = TF.to_tensor(inp_img)\n",
        "        tar_img = TF.to_tensor(tar_img)\n",
        "\n",
        "        hh, ww = tar_img.shape[1], tar_img.shape[2]\n",
        "\n",
        "        rr     = random.randint(0, hh-ps)\n",
        "        cc     = random.randint(0, ww-ps)\n",
        "        aug    = random.randint(0, 8)\n",
        "\n",
        "        # Crop patch\n",
        "        inp_img = inp_img[:, rr:rr+ps, cc:cc+ps]\n",
        "        tar_img = tar_img[:, rr:rr+ps, cc:cc+ps]\n",
        "\n",
        "        # Data Augmentations\n",
        "        if aug==1:\n",
        "            inp_img = inp_img.flip(1)\n",
        "            tar_img = tar_img.flip(1)\n",
        "        elif aug==2:\n",
        "            inp_img = inp_img.flip(2)\n",
        "            tar_img = tar_img.flip(2)\n",
        "        elif aug==3:\n",
        "            inp_img = torch.rot90(inp_img,dims=(1,2))\n",
        "            tar_img = torch.rot90(tar_img,dims=(1,2))\n",
        "        elif aug==4:\n",
        "            inp_img = torch.rot90(inp_img,dims=(1,2), k=2)\n",
        "            tar_img = torch.rot90(tar_img,dims=(1,2), k=2)\n",
        "        elif aug==5:\n",
        "            inp_img = torch.rot90(inp_img,dims=(1,2), k=3)\n",
        "            tar_img = torch.rot90(tar_img,dims=(1,2), k=3)\n",
        "        elif aug==6:\n",
        "            inp_img = torch.rot90(inp_img.flip(1),dims=(1,2))\n",
        "            tar_img = torch.rot90(tar_img.flip(1),dims=(1,2))\n",
        "        elif aug==7:\n",
        "            inp_img = torch.rot90(inp_img.flip(2),dims=(1,2))\n",
        "            tar_img = torch.rot90(tar_img.flip(2),dims=(1,2))\n",
        "\n",
        "        filename = os.path.splitext(os.path.split(tar_path)[-1])[0]\n",
        "\n",
        "        return tar_img, inp_img, filename\n",
        "\n",
        "class DataLoaderVal(Dataset):\n",
        "    def __init__(self, rgb_dir, img_options=None, rgb_dir2=None):\n",
        "        super(DataLoaderVal, self).__init__()\n",
        "\n",
        "        inp_files = sorted(os.listdir(os.path.join(rgb_dir, 'input')))\n",
        "        tar_files = sorted(os.listdir(os.path.join(rgb_dir, 'target')))\n",
        "\n",
        "        self.inp_filenames = [os.path.join(rgb_dir, 'input', x)  for x in inp_files if is_image_file(x)]\n",
        "        self.tar_filenames = [os.path.join(rgb_dir, 'target', x) for x in tar_files if is_image_file(x)]\n",
        "\n",
        "        self.img_options = img_options\n",
        "        self.sizex       = len(self.tar_filenames)  # get the size of target\n",
        "\n",
        "        self.ps = self.img_options['patch_size']\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.sizex\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        index_ = index % self.sizex\n",
        "        ps = self.ps\n",
        "\n",
        "        inp_path = self.inp_filenames[index_]\n",
        "        tar_path = self.tar_filenames[index_]\n",
        "\n",
        "        inp_img = Image.open(inp_path)\n",
        "        tar_img = Image.open(tar_path)\n",
        "\n",
        "        # Validate on center crop\n",
        "        if self.ps is not None:\n",
        "            inp_img = TF.center_crop(inp_img, (ps,ps))\n",
        "            tar_img = TF.center_crop(tar_img, (ps,ps))\n",
        "\n",
        "        inp_img = TF.to_tensor(inp_img)\n",
        "        tar_img = TF.to_tensor(tar_img)\n",
        "\n",
        "        filename = os.path.splitext(os.path.split(tar_path)[-1])[0]\n",
        "\n",
        "        return tar_img, inp_img, filename\n",
        "\n",
        "class DataLoaderTest(Dataset):\n",
        "    def __init__(self, inp_dir, img_options):\n",
        "        super(DataLoaderTest, self).__init__()\n",
        "\n",
        "        inp_files = sorted(os.listdir(inp_dir))\n",
        "        self.inp_filenames = [os.path.join(inp_dir, x) for x in inp_files if is_image_file(x)]\n",
        "\n",
        "        self.inp_size = len(self.inp_filenames)\n",
        "        self.img_options = img_options\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.inp_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        path_inp = self.inp_filenames[index]\n",
        "        filename = os.path.splitext(os.path.split(path_inp)[-1])[0]\n",
        "        inp = Image.open(path_inp)\n",
        "\n",
        "        inp = TF.to_tensor(inp)\n",
        "        return inp, filename"
      ],
      "metadata": {
        "id": "-V34tDnaZesg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class MixUp_AUG:\n",
        "    def __init__(self):\n",
        "        self.dist = torch.distributions.beta.Beta(torch.tensor([0.6]), torch.tensor([0.6]))\n",
        "\n",
        "    def aug(self, rgb_gt, rgb_noisy):\n",
        "        bs = rgb_gt.size(0)\n",
        "        indices = torch.randperm(bs)\n",
        "        rgb_gt2 = rgb_gt[indices]\n",
        "        rgb_noisy2 = rgb_noisy[indices]\n",
        "\n",
        "        lam = self.dist.rsample((bs,1)).view(-1,1,1,1).cuda()\n",
        "\n",
        "        rgb_gt    = lam * rgb_gt + (1-lam) * rgb_gt2\n",
        "        rgb_noisy = lam * rgb_noisy + (1-lam) * rgb_noisy2\n",
        "\n",
        "        return rgb_gt, rgb_noisy"
      ],
      "metadata": {
        "id": "TUHj9o_MZhl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install natsort"
      ],
      "metadata": {
        "id": "mA_GDRlVZkJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from natsort import natsorted\n",
        "from glob import glob\n",
        "\n",
        "def mkdirs(paths):\n",
        "    if isinstance(paths, list) and not isinstance(paths, str):\n",
        "        for path in paths:\n",
        "            mkdir(path)\n",
        "    else:\n",
        "        mkdir(paths)\n",
        "\n",
        "def mkdir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "def get_last_path(path, session):\n",
        "\tx = natsorted(glob(os.path.join(path,'*%s'%session)))[-1]\n",
        "\treturn x"
      ],
      "metadata": {
        "id": "8QXyLNSKZky9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python"
      ],
      "metadata": {
        "id": "capuHlwOZnOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def torchPSNR(tar_img, prd_img):\n",
        "    imdff = torch.clamp(prd_img,0,1) - torch.clamp(tar_img,0,1)\n",
        "    rmse = (imdff**2).mean().sqrt()\n",
        "    ps = 20*torch.log10(1/rmse)\n",
        "    return ps\n",
        "\n",
        "def save_img(filepath, img):\n",
        "    cv2.imwrite(filepath,cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "def numpyPSNR(tar_img, prd_img):\n",
        "    imdff = np.float32(prd_img) - np.float32(tar_img)\n",
        "    rmse = np.sqrt(np.mean(imdff**2))\n",
        "    ps = 20*np.log10(255/rmse)\n",
        "    return ps"
      ],
      "metadata": {
        "id": "slXbNFUsZqOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "\n",
        "def freeze(model):\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad=False\n",
        "\n",
        "def unfreeze(model):\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad=True\n",
        "\n",
        "def is_frozen(model):\n",
        "    x = [p.requires_grad for p in model.parameters()]\n",
        "    return not all(x)\n",
        "\n",
        "def save_checkpoint(model_dir, state, session):\n",
        "    epoch = state['epoch']\n",
        "    model_out_path = os.path.join(model_dir,\"model_epoch_{}_{}.pth\".format(epoch,session))\n",
        "    torch.save(state, model_out_path)\n",
        "\n",
        "def load_checkpoint(model, weights):\n",
        "    checkpoint = torch.load(weights)\n",
        "    try:\n",
        "        model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    except:\n",
        "        state_dict = checkpoint[\"state_dict\"]\n",
        "        new_state_dict = OrderedDict()\n",
        "        for k, v in state_dict.items():\n",
        "            name = k[7:] # remove `module.`\n",
        "            new_state_dict[name] = v\n",
        "        model.load_state_dict(new_state_dict)\n",
        "\n",
        "\n",
        "def load_checkpoint_multigpu(model, weights):\n",
        "    checkpoint = torch.load(weights)\n",
        "    state_dict = checkpoint[\"state_dict\"]\n",
        "    new_state_dict = OrderedDict()\n",
        "    for k, v in state_dict.items():\n",
        "        name = k[7:] # remove `module.`\n",
        "        new_state_dict[name] = v\n",
        "    model.load_state_dict(new_state_dict)\n",
        "\n",
        "def load_start_epoch(weights):\n",
        "    checkpoint = torch.load(weights)\n",
        "    epoch = checkpoint[\"epoch\"]\n",
        "    return epoch\n",
        "\n",
        "def load_optim(optimizer, weights):\n",
        "    checkpoint = torch.load(weights)\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    # for p in optimizer.param_groups: lr = p['lr']\n",
        "    # return lr"
      ],
      "metadata": {
        "id": "kmUhi_MmZtp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CharbonnierLoss(nn.Module):\n",
        "    \"\"\"Charbonnier Loss (L1)\"\"\"\n",
        "\n",
        "    def __init__(self, eps=1e-3):\n",
        "        super(CharbonnierLoss, self).__init__()\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        diff = x - y\n",
        "        # loss = torch.sum(torch.sqrt(diff * diff + self.eps))\n",
        "        loss = torch.mean(torch.sqrt((diff * diff) + (self.eps*self.eps)))\n",
        "        return loss\n",
        "\n",
        "class EdgeLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EdgeLoss, self).__init__()\n",
        "        k = torch.Tensor([[.05, .25, .4, .25, .05]])\n",
        "        self.kernel = torch.matmul(k.t(),k).unsqueeze(0).repeat(3,1,1,1)\n",
        "        if torch.cuda.is_available():\n",
        "            self.kernel = self.kernel.cuda()\n",
        "        self.loss = CharbonnierLoss()\n",
        "\n",
        "    def conv_gauss(self, img):\n",
        "        n_channels, _, kw, kh = self.kernel.shape\n",
        "        img = F.pad(img, (kw//2, kh//2, kw//2, kh//2), mode='replicate')\n",
        "        return F.conv2d(img, self.kernel, groups=n_channels)\n",
        "\n",
        "    def laplacian_kernel(self, current):\n",
        "        filtered    = self.conv_gauss(current)    # filter\n",
        "        down        = filtered[:,:,::2,::2]               # downsample\n",
        "        new_filter  = torch.zeros_like(filtered)\n",
        "        new_filter[:,:,::2,::2] = down*4                  # upsample\n",
        "        filtered    = self.conv_gauss(new_filter) # filter\n",
        "        diff = current - filtered\n",
        "        return diff\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        loss = self.loss(self.laplacian_kernel(x), self.laplacian_kernel(y))\n",
        "        return loss"
      ],
      "metadata": {
        "id": "HengKXxEZxWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install warmup-scheduler"
      ],
      "metadata": {
        "id": "MuWvMKsxZzxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "from warmup_scheduler import GradualWarmupScheduler\n",
        "######### Set Seeds ###########\n",
        "random.seed(4321)\n",
        "np.random.seed(1234)\n",
        "torch.manual_seed(1234)\n",
        "torch.cuda.manual_seed_all(1234)\n",
        "\n",
        "start_epoch = 1\n",
        "class Config:\n",
        "    GPU = [0, 1, 2, 3]\n",
        "    VERBOSE = True\n",
        "\n",
        "    class MODEL:\n",
        "        MODE = 'Deblurring'\n",
        "        SESSION = 'DGUNet'\n",
        "\n",
        "    class OPTIM:\n",
        "        BATCH_SIZE = 1\n",
        "        NUM_EPOCHS = 50\n",
        "        # NEPOCH_DECAY = [10]\n",
        "        LR_INITIAL = 1e-4\n",
        "        LR_MIN = 1e-6\n",
        "        # BETA1 = 0.9\n",
        "\n",
        "    class TRAINING:\n",
        "        VAL_AFTER_EVERY = 10\n",
        "        RESUME = False\n",
        "        TRAIN_PS = 256\n",
        "        VAL_PS = 256\n",
        "opt=Config()\n",
        "\n",
        "result_dir = \"./drive/MyDrive/ImageRestoration/results\"\n",
        "trained_dir  = \"./drive/MyDrive/ImageRestoration/m/Generator.pth\"\n",
        "model_dir  = \"./drive/MyDrive/ImageRestoration/m\"\n",
        "\n",
        "\n",
        "train_dir = \"/content/drive/MyDrive/Major/References/lol_dataset/our485\"\n",
        "val_dir   = \"./value\"\n",
        "\n",
        "######### Model ###########\n",
        "model_restoration = Generator()\n",
        "model_restoration.cuda()\n",
        "\n",
        "checkpoint = torch.load(trained_dir)\n",
        "try:\n",
        "    model_restoration.load_state_dict(checkpoint[\"state_dict\"])\n",
        "except:\n",
        "    state_dict = checkpoint[\"state_dict\"]\n",
        "    new_state_dict = OrderedDict()\n",
        "    for k, v in state_dict.items():\n",
        "        name = k[7:]  # remove `module.`\n",
        "        new_state_dict[name] = v\n",
        "    model_restoration.load_state_dict(new_state_dict)\n",
        "######### Optimizer ###########\n",
        "\n",
        "new_lr=opt.OPTIM.LR_INITIAL\n",
        "\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model_restoration.parameters(), lr=new_lr, betas=(0.9, 0.999),eps=1e-8)\n",
        "\n",
        "\n",
        "######### Scheduler ###########\n",
        "warmup_epochs = 3\n",
        "scheduler_cosine = optim.lr_scheduler.CosineAnnealingLR(optimizer, warmup_epochs, eta_min=opt.OPTIM.LR_MIN)\n",
        "scheduler = GradualWarmupScheduler(optimizer, multiplier=1, total_epoch=warmup_epochs, after_scheduler=scheduler_cosine)\n",
        "scheduler.step()\n",
        "\n",
        "\n",
        "for i in range(1, start_epoch):\n",
        "    scheduler.step()\n",
        "    new_lr = scheduler.get_lr()[0]\n",
        "    print('------------------------------------------------------------------------------')\n",
        "    print(\"==> Resuming Training with learning rate:\", new_lr)\n",
        "    print('------------------------------------------------------------------------------')\n",
        "\n",
        "#Loss\n",
        "criterion_char = CharbonnierLoss()\n",
        "criterion_edge = EdgeLoss()\n",
        "\n",
        "# DataLoaders\n",
        "train_dataset = LOLDataset(train_dir)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=opt.OPTIM.BATCH_SIZE, shuffle=True, num_workers=16, drop_last=False, pin_memory=True)\n",
        "\n",
        "\n",
        "\n",
        "print('===> Start Epoch {} End Epoch {}'.format(start_epoch,opt.OPTIM.NUM_EPOCHS + 1))\n",
        "print('===> Loading datasets')\n",
        "\n",
        "best_psnr = 0\n",
        "best_epoch = 0\n",
        "\n",
        "for epoch in range(start_epoch, opt.OPTIM.NUM_EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    epoch_loss = 0\n",
        "    train_id = 1\n",
        "\n",
        "    model_restoration.train()\n",
        "    for i, data in enumerate(tqdm(train_loader), 0):\n",
        "\n",
        "        # zero_grad\n",
        "        for param in model_restoration.parameters():\n",
        "            param.grad = None\n",
        "\n",
        "        target = data[1].cuda()\n",
        "        input_ = data[0].cuda()\n",
        "\n",
        "        restored = model_restoration(input_)\n",
        "        # Compute loss at each stage\n",
        "        # Compute loss at each stage (detach tensors before converting to NumPy)\n",
        "        loss_char = sum([criterion_char(restored[j], target) for j in range(len(restored))])\n",
        "\n",
        "        # Compute loss at each stage using PyTorch's sum function (instead of np.sum)\n",
        "        loss_char = sum([criterion_char(restored[j], target) for j in range(len(restored))])\n",
        "        loss_edge = sum([criterion_edge(restored[j], target) for j in range(len(restored))])\n",
        "\n",
        "# Combine losses\n",
        "        loss = loss_char + (0.05 * loss_edge)\n",
        "\n",
        "# Backpropagation and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()  # Convert loss tensor to scalar for accumulation\n",
        "\n",
        "\n",
        "        torch.save({'epoch': epoch,\n",
        "                    'state_dict': model_restoration.state_dict(),\n",
        "                    'optimizer' : optimizer.state_dict()\n",
        "                    }, os.path.join(model_dir,f\"model_epoch_{epoch}.pth\"))\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    print(\"------------------------------------------------------------------\")\n",
        "    print(\"Epoch: {}\\tTime: {:.4f}\\tLoss: {:.4f}\\tLearningRate {:.6f}\".format(epoch, time.time()-epoch_start_time, epoch_loss, scheduler.get_lr()[0]))\n",
        "    print(\"------------------------------------------------------------------\")\n",
        "\n",
        "    torch.save({'epoch': epoch,\n",
        "                'state_dict': model_restoration.state_dict(),\n",
        "                'optimizer' : optimizer.state_dict()\n",
        "                }, os.path.join(model_dir,\"model_latest.pth\"))"
      ],
      "metadata": {
        "id": "adDAY3P3Z2jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import argparse\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from collections import OrderedDict\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from skimage import img_as_ubyte\n",
        "\n",
        "class Config:\n",
        "    input_dir = '/content/drive/MyDrive/Major/ImageRestoration/deblur_input'\n",
        "    result_dir = './drive/MyDrive/Major/ImageRestoration/output_deblur'\n",
        "    weights = './drive/MyDrive/Major/ImageRestoration/m/Deblurring/Generator.pth'\n",
        "    dataset = 'GoPro'\n",
        "    patch_size = 256\n",
        "    stride = 256\n",
        "\n",
        "args = Config()\n",
        "\n",
        "# Load Model\n",
        "model = Generator()\n",
        "checkpoint = torch.load(args.weights)\n",
        "state_dict = checkpoint[\"state_dict\"]\n",
        "new_state_dict = OrderedDict()\n",
        "for k, v in state_dict.items():\n",
        "    name = k[7:]  # remove `module.`\n",
        "    new_state_dict[name] = v\n",
        "model.load_state_dict(new_state_dict)\n",
        "\n",
        "print(\"===> Testing using weights:\", args.weights)\n",
        "model.cuda()\n",
        "model = nn.DataParallel(model)\n",
        "model.eval()\n",
        "\n",
        "# Dataset Loader\n",
        "test_dataset = GoProDataset(root_dir=args.input_dir)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False, num_workers=4, drop_last=False, pin_memory=True)\n",
        "\n",
        "# Padding Function (Handles Small Images)\n",
        "def pad_image(img, patch_size=256):\n",
        "    _, _, h, w = img.shape\n",
        "    pad_h = (patch_size - h % patch_size) % patch_size\n",
        "    pad_w = (patch_size - w % patch_size) % patch_size\n",
        "    padded_img = F.pad(img, (0, pad_w, 0, pad_h), mode='reflect')  # Reflective padding to minimize artifacts\n",
        "    return padded_img, pad_h, pad_w\n",
        "\n",
        "# Patch Extraction Function\n",
        "def extract_patches(img, patch_size=256, stride=256):\n",
        "    _, _, h, w = img.shape\n",
        "    unfold = F.unfold(img, kernel_size=patch_size, stride=stride)\n",
        "    patches = unfold.view(img.size(0), img.size(1), patch_size, patch_size, -1)\n",
        "    return patches, h, w\n",
        "\n",
        "# Patch Merging Function\n",
        "def merge_patches(patches, orig_h, orig_w, patch_size=256, stride=256):\n",
        "    patches = torch.cat(patches, dim=0)\n",
        "    fold = F.fold(patches, output_size=(orig_h, orig_w), kernel_size=patch_size, stride=stride)\n",
        "    return fold\n",
        "\n",
        "# Processing Images\n",
        "with torch.no_grad():\n",
        "    for i, data_test in enumerate(tqdm(test_loader), 0):\n",
        "        torch.cuda.ipc_collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        input_ = data_test[0].cuda()\n",
        "        filenames = data_test[1]\n",
        "\n",
        "        # Apply Padding if Needed\n",
        "        input_, pad_h, pad_w = pad_image(input_, patch_size=args.patch_size)\n",
        "\n",
        "        # Extract patches\n",
        "        patches, orig_h, orig_w = extract_patches(input_, patch_size=args.patch_size, stride=args.stride)\n",
        "        restored_patches = []\n",
        "\n",
        "        for j in range(patches.shape[-1]):\n",
        "            patch = patches[..., j].cuda()\n",
        "            restored_patch = model(patch.unsqueeze(0))\n",
        "            restored_patch = torch.clamp(restored_patch, 0, 1)\n",
        "            restored_patches.append(restored_patch)\n",
        "\n",
        "        # Merge patches back\n",
        "        restored = merge_patches(restored_patches, orig_h, orig_w, patch_size=args.patch_size, stride=args.stride)\n",
        "\n",
        "        # Remove Padding Before Saving\n",
        "        restored = restored[:, :, :orig_h - pad_h, :orig_w - pad_w]\n",
        "\n",
        "        # Save the final image\n",
        "        restored_im = img_as_ubyte(restored.cpu().squeeze().numpy())\n",
        "        cv2.imwrite(os.path.join(args.result_dir, filenames[0].split(\"/\")[-1]), cv2.cvtColor(restored_im, cv2.COLOR_RGB2BGR))\n",
        "        print(\"Saved:\", os.path.join(args.result_dir, filenames[0].split(\"/\")[-1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "O-BGZHdfptpS",
        "outputId": "fbd5d818-76f6-4c20-b54f-571d423b69d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===> Testing using weights: ./drive/MyDrive/Major/ImageRestoration/m/Deblurring/Generator.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/2 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 1, 3, 256, 256]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-ba6a8686e51a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mpatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mrestored_patch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0mrestored_patch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestored_patch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mrestored_patches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestored_patch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-d702bdf02fe1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0mphixsy_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphi_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0mx1_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr0\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphit_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphixsy_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshallow_feat1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-d702bdf02fe1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 1, 3, 256, 256]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import argparse\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from collections import OrderedDict\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from skimage import img_as_ubyte\n",
        "\n",
        "class Config:\n",
        "    input_dir = '/content/drive/MyDrive/ImageRestoration/deblur_input'\n",
        "    result_dir = './drive/MyDrive/ImageRestoration/output_deblur'\n",
        "    weights = './drive/MyDrive/ImageRestoration/m/Deblurring/Generator.pth'\n",
        "    dataset = 'GoPro'\n",
        "    patch_size = 256\n",
        "    stride = 256\n",
        "\n",
        "args = Config()\n",
        "\n",
        "# Load Model\n",
        "model = Generator()\n",
        "checkpoint = torch.load(args.weights)\n",
        "state_dict = checkpoint[\"state_dict\"]\n",
        "new_state_dict = OrderedDict()\n",
        "for k, v in state_dict.items():\n",
        "    name = k[7:]  # remove `module.`\n",
        "    new_state_dict[name] = v\n",
        "model.load_state_dict(new_state_dict)\n",
        "\n",
        "print(\"===> Testing using weights:\", args.weights)\n",
        "model.cuda()\n",
        "model = nn.DataParallel(model)\n",
        "model.eval()\n",
        "\n",
        "# Dataset Loader\n",
        "test_dataset = GoProDataset(root_dir=args.input_dir)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False, num_workers=4, drop_last=False, pin_memory=True)\n",
        "\n",
        "# Patch Extraction Function\n",
        "def extract_patches(img, patch_size=256, stride=256):\n",
        "    _, _, h, w = img.shape\n",
        "    unfold = F.unfold(img, kernel_size=patch_size, stride=stride)\n",
        "    patches = unfold.view(img.size(0), img.size(1), patch_size, patch_size, -1)\n",
        "    return patches, h, w\n",
        "\n",
        "# Patch Merging Function\n",
        "def merge_patches(patches, orig_h, orig_w, patch_size=256, stride=256):\n",
        "    patches = torch.cat(patches, dim=0)\n",
        "    fold = F.fold(patches, output_size=(orig_h, orig_w), kernel_size=patch_size, stride=stride)\n",
        "    return fold\n",
        "\n",
        "# Processing Images\n",
        "with torch.no_grad():\n",
        "    for i, data_test in enumerate(tqdm(test_loader), 0):\n",
        "        torch.cuda.ipc_collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        input_ = data_test[0].cuda()\n",
        "        filenames = data_test[1]\n",
        "\n",
        "        # Extract patches\n",
        "        patches, orig_h, orig_w = extract_patches(input_, patch_size=args.patch_size, stride=args.stride)\n",
        "        restored_patches = []\n",
        "\n",
        "        for j in range(patches.shape[-1]):\n",
        "            patch = patches[..., j].cuda()\n",
        "            restored_patch = model(patch.unsqueeze(0))\n",
        "            restored_patch = torch.clamp(restored_patch, 0, 1)\n",
        "            restored_patches.append(restored_patch)\n",
        "\n",
        "        # Merge patches back\n",
        "        restored = merge_patches(restored_patches, orig_h, orig_w, patch_size=args.patch_size, stride=args.stride)\n",
        "\n",
        "        # Save the final image\n",
        "        restored_im = img_as_ubyte(restored.cpu().squeeze().numpy())\n",
        "        cv2.imwrite(os.path.join(args.result_dir, filenames[0].split(\"/\")[-1]), cv2.cvtColor(restored_im, cv2.COLOR_RGB2BGR))\n",
        "        print(\"Saved:\", os.path.join(args.result_dir, filenames[0].split(\"/\")[-1]))"
      ],
      "metadata": {
        "id": "DpEisdKKpTor"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}