{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "collapsed_sections": [
        "U5J8Z5oieMYk",
        "ZrXZFa9aeVJi"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Low-Light**"
      ],
      "metadata": {
        "id": "U5J8Z5oieMYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas openpyxl scikit-image pytorch-ssim"
      ],
      "metadata": {
        "id": "LmezgvG1XRiF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01eae521-e329-47b7-cbc6-5d10b841b1d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Collecting pytorch-ssim\n",
            "  Downloading pytorch_ssim-0.1.tar.gz (1.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (1.15.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (3.4.2)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (11.2.1)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.3.30)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Building wheels for collected packages: pytorch-ssim\n",
            "  Building wheel for pytorch-ssim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-ssim: filename=pytorch_ssim-0.1-py3-none-any.whl size=2006 sha256=d82a67a2007bcf93f9c99038b4aa3b71bebf72cd362c5be81a3af504523dc007\n",
            "  Stored in directory: /root/.cache/pip/wheels/58/68/a2/68a41e8268a076c128bbc3988d243187fa4681828e648bf1ca\n",
            "Successfully built pytorch-ssim\n",
            "Installing collected packages: pytorch-ssim\n",
            "Successfully installed pytorch-ssim-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30sh-h5SQ0N7"
      },
      "outputs": [],
      "source": [
        "import pytorch_ssim\n",
        "import torch.nn as nn\n",
        "class CGSformerLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.7, beta=0.3):\n",
        "        super(CGSformerLoss, self).__init__()\n",
        "        self.mse = nn.MSELoss()\n",
        "        self.ssim = pytorch_ssim.SSIM(window_size=11)\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "\n",
        "    def forward(self, output, target):\n",
        "        mse_loss = self.mse(output, target)\n",
        "        ssim_loss = 1 - self.ssim(output, target)\n",
        "        return self.alpha * mse_loss + self.beta * ssim_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "import random\n",
        "\n",
        "class LOLDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None,patch_size=128):\n",
        "        self.low_light_dir = os.path.join(root_dir, 'low')\n",
        "        self.high_light_dir = os.path.join(root_dir, 'high')\n",
        "        self.low_light_images = sorted(os.listdir(self.low_light_dir))\n",
        "        self.high_light_images = sorted(os.listdir(self.high_light_dir))\n",
        "        self.transform = transform\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.low_light_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        low_image_path = os.path.join(self.low_light_dir, self.low_light_images[idx])\n",
        "        high_image_path = os.path.join(self.high_light_dir, self.high_light_images[idx])\n",
        "\n",
        "        low_img = Image.open(low_image_path).convert('RGB')\n",
        "        high_img = Image.open(high_image_path).convert('RGB')\n",
        "\n",
        "        # Random crop\n",
        "        i, j, h, w = transforms.RandomCrop.get_params(low_img, output_size=(self.patch_size, self.patch_size))\n",
        "        low_img = transforms.functional.crop(low_img, i, j, h, w)\n",
        "        high_img = transforms.functional.crop(high_img, i, j, h, w)\n",
        "\n",
        "        # Random flip\n",
        "        if random.random() > 0.5:\n",
        "            low_img = transforms.functional.hflip(low_img)\n",
        "            high_img = transforms.functional.hflip(high_img)\n",
        "        if random.random() > 0.5:\n",
        "            low_img = transforms.functional.vflip(low_img)\n",
        "            high_img = transforms.functional.vflip(high_img)\n",
        "\n",
        "        if self.transform:\n",
        "            low_img = self.transform(low_img)\n",
        "            high_img = self.transform(high_img)\n",
        "\n",
        "        return low_img, high_img\n",
        "\n",
        "# Example transforms\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "k4acDdrYSMAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CFS(nn.Module): #Cross Feature Scrambling\n",
        "    def __init__(self, channels, threshold=0.5):\n",
        "        super(CFS, self).__init__()\n",
        "        self.threshold = threshold\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.gn = nn.GroupNorm(1, channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_ln = self.gn(x)\n",
        "        var = torch.var(x_ln, dim=[2,3], keepdim=True)\n",
        "        importance = var / (torch.sum(var, dim=1, keepdim=True) + 1e-6)\n",
        "        importance = self.sigmoid(importance)\n",
        "        mask_info = (importance > self.threshold).float()\n",
        "        mask_noninfo = (importance <= self.threshold).float()\n",
        "\n",
        "        x_info = mask_info * x_ln\n",
        "        x_noninfo = mask_noninfo * x_ln\n",
        "\n",
        "        pooled = F.adaptive_avg_pool2d(x_info + x_noninfo, (1, 1))\n",
        "        beta = self.sigmoid(pooled)\n",
        "\n",
        "        out = beta * x_info + (1 - beta) * x_noninfo\n",
        "        return out\n",
        "\n",
        "class ASA(nn.Module): #Adaptive Shift Attention\n",
        "    def __init__(self, channels, topk_ratio=0.5):\n",
        "        super(ASA, self).__init__()\n",
        "        self.topk_ratio = topk_ratio\n",
        "        self.query_conv = nn.Conv2d(channels, channels, 1)\n",
        "        self.key_conv = nn.Conv2d(channels, channels, 1)\n",
        "        self.value_conv = nn.Conv2d(channels, channels, 1)\n",
        "        self.scale = channels ** -0.5\n",
        "\n",
        "    def forward(self, x):\n",
        "        q = self.query_conv(x).flatten(2).transpose(1, 2)\n",
        "        k = self.key_conv(x).flatten(2).transpose(1, 2)\n",
        "        v = self.value_conv(x).flatten(2).transpose(1, 2)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        topk = int(attn.size(-1) * self.topk_ratio)\n",
        "        topk_values, _ = torch.topk(attn, k=topk, dim=-1)\n",
        "        threshold = topk_values[:, :, -1].unsqueeze(-1)\n",
        "        mask = attn >= threshold\n",
        "        attn = attn.masked_fill(~mask, float('-inf'))\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "\n",
        "        out = attn @ v\n",
        "        out = out.transpose(1, 2).reshape(x.size())\n",
        "        return out\n",
        "\n",
        "class BGFF(nn.Module): #BIlateral Grid Feature Fusion\n",
        "    def __init__(self, channels):\n",
        "        super(BGFF, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, 1)\n",
        "        self.conv_dw3x3 = nn.Conv2d(channels, channels, 3, padding=1, groups=channels)\n",
        "        self.conv_dw7x7 = nn.Conv2d(channels, channels, 7, padding=3, groups=channels)\n",
        "        self.conv2 = nn.Conv2d(channels, channels, 1)\n",
        "        self.swish = lambda x: x * torch.sigmoid(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        path1 = self.swish(self.conv_dw3x3(out))\n",
        "        path2 = self.swish(self.conv_dw7x7(out))\n",
        "        out = path1 * path2\n",
        "        out = self.conv2(out)\n",
        "        return out + x\n",
        "\n",
        "class CGSformerBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(CGSformerBlock, self).__init__()\n",
        "        self.cfs = CFS(channels)\n",
        "        self.asa = ASA(channels)\n",
        "        self.bgff = BGFF(channels)\n",
        "        self.norm1 = nn.LayerNorm([channels, 128, 128])\n",
        "        self.norm2 = nn.LayerNorm([channels, 128, 128])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_cfs = self.cfs(x)\n",
        "        x = self.asa(self.norm1(x_cfs)) + x\n",
        "        x = self.bgff(self.norm2(x)) + x\n",
        "        return x\n",
        "\n",
        "class SparseTransformer(nn.Module):\n",
        "    def __init__(self, channels=64):\n",
        "        super(SparseTransformer, self).__init__()\n",
        "        self.encoder = nn.Conv2d(3, channels, 3, padding=1)\n",
        "\n",
        "        self.block1 = CGSformerBlock(channels)\n",
        "        self.block2 = CGSformerBlock(channels)\n",
        "        self.block3 = CGSformerBlock(channels)\n",
        "        self.block4 = CGSformerBlock(channels)\n",
        "        self.block5 = CGSformerBlock(channels)\n",
        "        self.block6 = CGSformerBlock(channels)\n",
        "        self.block7 = CGSformerBlock(channels)\n",
        "\n",
        "        self.decoder = nn.Conv2d(channels, 3, 3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "        x = self.block5(x)\n",
        "        x = self.block6(x)\n",
        "        x = self.block7(x)\n",
        "\n",
        "        x = self.decoder(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "solNw7jjR7iA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def ssim(img1, img2, window_size=11):\n",
        "    channel = img1.shape[1]\n",
        "    window = torch.ones((channel, 1, window_size, window_size)).to(img1.device) / (window_size ** 2)\n",
        "\n",
        "    mu1 = F.conv2d(img1, window, padding=window_size // 2, groups=channel)\n",
        "    mu2 = F.conv2d(img2, window, padding=window_size // 2, groups=channel)\n",
        "\n",
        "    sigma1_sq = F.conv2d(img1 ** 2, window, padding=window_size // 2, groups=channel) - mu1 ** 2\n",
        "    sigma2_sq = F.conv2d(img2 ** 2, window, padding=window_size // 2, groups=channel) - mu2 ** 2\n",
        "    sigma12 = F.conv2d(img1 * img2, window, padding=window_size // 2, groups=channel) - mu1 * mu2\n",
        "\n",
        "    C1, C2 = 0.01**2, 0.03**2  # Stability constants\n",
        "    ssim_map = ((2 * mu1 * mu2 + C1) * (2 * sigma12 + C2)) / ((mu1 ** 2 + mu2 ** 2 + C1) * (sigma1_sq + sigma2_sq + C2))\n",
        "\n",
        "    return ssim_map.mean()\n",
        "\n"
      ],
      "metadata": {
        "id": "piZ3892smGHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing**"
      ],
      "metadata": {
        "id": "ZrXZFa9aeVJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional as TF\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "def pad_to_multiple(img, patch_size=128):\n",
        "    \"\"\" Pad the image to multiple of patch size (no less than original size) \"\"\"\n",
        "    _, h, w = img.shape\n",
        "    pad_h = (patch_size - h % patch_size) % patch_size\n",
        "    pad_w = (patch_size - w % patch_size) % patch_size\n",
        "    img = F.pad(img, (0, pad_w, 0, pad_h), mode='reflect')\n",
        "    return img\n",
        "\n",
        "def split_patches(img, patch_size=128):\n",
        "    \"\"\" Split the image into non-overlapping patches \"\"\"\n",
        "    patches = []\n",
        "    coords = []\n",
        "    c, h, w = img.shape\n",
        "    for i in range(0, h, patch_size):\n",
        "        for j in range(0, w, patch_size):\n",
        "            patch = img[:, i:i+patch_size, j:j+patch_size]\n",
        "            patches.append(patch)\n",
        "            coords.append((i, j))\n",
        "    return patches, coords\n",
        "\n",
        "def merge_patches(patches, coords, image_shape, patch_size=128):\n",
        "    \"\"\" Merge patches back into full image \"\"\"\n",
        "    c, h, w = image_shape\n",
        "    merged = torch.zeros((c, h, w)).to(patches[0].device)\n",
        "    counter = torch.zeros((c, h, w)).to(patches[0].device)\n",
        "\n",
        "    for patch, (i, j) in zip(patches, coords):\n",
        "        merged[:, i:i+patch.shape[1], j:j+patch.shape[2]] += patch\n",
        "        counter[:, i:i+patch.shape[1], j:j+patch.shape[2]] += 1\n",
        "\n",
        "    counter[counter == 0] = 1\n",
        "    merged = merged / counter\n",
        "    return merged\n",
        "\n",
        "def enhance_image(model, img_path, save_path, device, patch_size=128):\n",
        "    \"\"\" Full enhancement pipeline \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Load image\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    img_tensor = TF.to_tensor(img).to(device)\n",
        "\n",
        "    c, h, w = img_tensor.shape\n",
        "\n",
        "    if h < patch_size or w < patch_size:\n",
        "        # If image is smaller in any dimension, pad to at least 128\n",
        "        img_tensor = pad_to_multiple(img_tensor, patch_size)\n",
        "        with torch.no_grad():\n",
        "            output = model(img_tensor.unsqueeze(0)).squeeze(0)\n",
        "        output = output[:, :h, :w]  # Crop back to original size\n",
        "    else:\n",
        "        # Normal size or large image\n",
        "        padded_img = pad_to_multiple(img_tensor, patch_size)\n",
        "        c_pad, h_pad, w_pad = padded_img.shape\n",
        "\n",
        "        patches, coords = split_patches(padded_img, patch_size)\n",
        "\n",
        "        enhanced_patches = []\n",
        "        with torch.no_grad():\n",
        "            for patch in patches:\n",
        "                out_patch = model(patch.unsqueeze(0)).squeeze(0)\n",
        "                enhanced_patches.append(out_patch)\n",
        "\n",
        "        merged = merge_patches(enhanced_patches, coords, (c_pad, h_pad, w_pad), patch_size)\n",
        "        output = merged[:, :h, :w]  # Remove padding to original size\n",
        "\n",
        "    output_img = TF.to_pil_image(torch.clamp(output, 0, 1).cpu())\n",
        "    output_img.save(save_path)\n",
        "    print(f\"Saved enhanced image at {save_path}\")\n"
      ],
      "metadata": {
        "id": "Ne__xx6BADHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "test_images_dir = \"./drive/MyDrive/Major/ImageRestoration/low_input\"\n",
        "save_dir = \"./drive/MyDrive/Major/ImageRestoration/m/Sparse_transform\"\n",
        "model = SparseTransformer().to(device)\n",
        "model.load_state_dict(torch.load(save_dir+\"/model_epoch_100.pth\",map_location=device))\n",
        "out_dir=\"./drive/MyDrive/Major/ImageRestoration/output_low\"\n",
        "\n",
        "for img_name in os.listdir(test_images_dir):\n",
        "    img_path = os.path.join(test_images_dir, img_name)\n",
        "    save_path = os.path.join(out_dir, img_name)\n",
        "    enhance_image(model, img_path, save_path, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqOEEs_cAL2J",
        "outputId": "55e90837-3de1-4fcb-e0de-81d235bb0145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved enhanced image at ./drive/MyDrive/Major/ImageRestoration/output_low/669.png\n",
            "Saved enhanced image at ./drive/MyDrive/Major/ImageRestoration/output_low/1.png\n",
            "Saved enhanced image at ./drive/MyDrive/Major/ImageRestoration/output_low/778.png\n",
            "Saved enhanced image at ./drive/MyDrive/Major/ImageRestoration/output_low/179.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "HI4VhwpOeZMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import pytorch_ssim\n",
        "from collections import OrderedDict\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load LOL Dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "num_epochs = 150\n",
        "start_epoch = 100\n",
        "batch_size = 1\n",
        "patch_size = 128\n",
        "initial_lr = 3e-4\n",
        "\n",
        "save_dir = \"./drive/MyDrive/Major/ImageRestoration/m/Sparse_transform\"\n",
        "log_dir = \"./drive/MyDrive/Major/ImageRestoration/training_log\"\n",
        "\n",
        "train_dataset = LOLDataset(root_dir=\"./drive/MyDrive/Major/References/lol_dataset/our485\", transform=transforms.ToTensor(),patch_size=patch_size)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "# Initialize model, loss function, optimizer\n",
        "model = SparseTransformer().cuda()\n",
        "if start_epoch!=0:\n",
        "  checkpoint_path = f\"./drive/MyDrive/Major/ImageRestoration/m/Sparse_transform/model_epoch_{start_epoch}.pth\"\n",
        "  checkpoint = torch.load(checkpoint_path)\n",
        "  try:\n",
        "      model.load_state_dict(checkpoint)\n",
        "  except:\n",
        "      state_dict = checkpoint\n",
        "      new_state_dict = OrderedDict()\n",
        "      for k, v in state_dict.items():\n",
        "        name = k[7:]  # remove `module.`\n",
        "        new_state_dict[name] = v\n",
        "      model.load_state_dict(new_state_dict)\n",
        "criterion_mse = torch.nn.MSELoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=3e-4)\n",
        "\n",
        "# Training loop\n",
        "log_results = []\n",
        "\n",
        "def compute_psnr(target, output):\n",
        "    mse = F.mse_loss(target, output)\n",
        "    psnr = 20 * torch.log10(1.0 / torch.sqrt(mse))\n",
        "    return psnr.item()\n",
        "\n",
        "for epoch in range(start_epoch,num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_psnr = 0\n",
        "    epoch_ssim = 0\n",
        "\n",
        "    for i_img,o_img  in train_loader:\n",
        "        i_img = i_img.cuda()\n",
        "        o_img = o_img.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        enhanced_img = model(i_img)\n",
        "\n",
        "        loss_mse = criterion_mse(enhanced_img, o_img)\n",
        "        loss_ssim = ssim(enhanced_img, o_img)\n",
        "        loss = 0.7 * loss_mse + 0.3 * (1 - loss_ssim)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_psnr += compute_psnr(o_img, enhanced_img)\n",
        "        epoch_ssim += loss_ssim.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "    avg_psnr = epoch_psnr / len(train_loader)\n",
        "    avg_ssim = epoch_ssim / len(train_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, PSNR: {avg_psnr:.4f}, SSIM: {avg_ssim:.4f}\")\n",
        "\n",
        "    # Save model every epoch\n",
        "    model_path = os.path.join(save_dir, f\"model_epoch_{epoch+1}.pth\")\n",
        "    torch.save(model.state_dict(),model_path)\n",
        "\n",
        "    # Logging to Excel\n",
        "    log_results.append({\"Epoch\": epoch+1, \"Loss\": avg_loss, \"PSNR\": avg_psnr, \"SSIM\": avg_ssim})\n",
        "\n",
        "    df = pd.DataFrame(log_results)\n",
        "    df.to_excel(os.path.join(log_dir,\"training_log_100.xlsx\"), index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccEtgxeCfj6W",
        "outputId": "540e7655-376d-493c-f235-851d5fc55539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 101/150, Loss: 0.0657, PSNR: 19.6254, SSIM: 0.8280\n",
            "Epoch 102/150, Loss: 0.0670, PSNR: 19.3574, SSIM: 0.8244\n",
            "Epoch 103/150, Loss: 0.0651, PSNR: 19.5050, SSIM: 0.8291\n",
            "Epoch 104/150, Loss: 0.0640, PSNR: 19.6134, SSIM: 0.8310\n",
            "Epoch 105/150, Loss: 0.0661, PSNR: 19.6908, SSIM: 0.8240\n",
            "Epoch 106/150, Loss: 0.0685, PSNR: 19.3740, SSIM: 0.8196\n",
            "Epoch 107/150, Loss: 0.0705, PSNR: 19.1405, SSIM: 0.8130\n",
            "Epoch 108/150, Loss: 0.0641, PSNR: 19.8280, SSIM: 0.8281\n",
            "Epoch 109/150, Loss: 0.0659, PSNR: 19.5201, SSIM: 0.8263\n",
            "Epoch 110/150, Loss: 0.0630, PSNR: 19.6569, SSIM: 0.8350\n",
            "Epoch 111/150, Loss: 0.0660, PSNR: 19.6793, SSIM: 0.8240\n",
            "Epoch 112/150, Loss: 0.0696, PSNR: 19.2604, SSIM: 0.8166\n",
            "Epoch 113/150, Loss: 0.0659, PSNR: 19.5152, SSIM: 0.8265\n",
            "Epoch 114/150, Loss: 0.0639, PSNR: 19.6099, SSIM: 0.8290\n",
            "Epoch 115/150, Loss: 0.0634, PSNR: 19.7042, SSIM: 0.8319\n",
            "Epoch 116/150, Loss: 0.0677, PSNR: 19.3560, SSIM: 0.8215\n",
            "Epoch 117/150, Loss: 0.0646, PSNR: 19.6451, SSIM: 0.8292\n",
            "Epoch 118/150, Loss: 0.0615, PSNR: 20.0499, SSIM: 0.8358\n",
            "Epoch 119/150, Loss: 0.0680, PSNR: 19.6888, SSIM: 0.8187\n",
            "Epoch 120/150, Loss: 0.0705, PSNR: 19.0257, SSIM: 0.8140\n",
            "Epoch 121/150, Loss: 0.0654, PSNR: 19.9922, SSIM: 0.8243\n",
            "Epoch 122/150, Loss: 0.0633, PSNR: 19.8028, SSIM: 0.8305\n",
            "Epoch 123/150, Loss: 0.0644, PSNR: 19.7350, SSIM: 0.8297\n",
            "Epoch 124/150, Loss: 0.0643, PSNR: 19.9114, SSIM: 0.8281\n",
            "Epoch 125/150, Loss: 0.0635, PSNR: 19.9628, SSIM: 0.8301\n",
            "Epoch 126/150, Loss: 0.0682, PSNR: 19.2987, SSIM: 0.8196\n",
            "Epoch 127/150, Loss: 0.0653, PSNR: 19.6287, SSIM: 0.8263\n"
          ]
        }
      ]
    }
  ]
}