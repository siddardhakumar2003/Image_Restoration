{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31d82db9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T10:40:02.774240Z",
     "iopub.status.busy": "2025-05-14T10:40:02.773972Z",
     "iopub.status.idle": "2025-05-14T10:40:09.020989Z",
     "shell.execute_reply": "2025-05-14T10:40:09.020096Z"
    },
    "id": "t4rBqXvZtptG",
    "papermill": {
     "duration": 6.253454,
     "end_time": "2025-05-14T10:40:09.022507",
     "exception": false,
     "start_time": "2025-05-14T10:40:02.769053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\r\n",
      "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\r\n",
      "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.25.2)\r\n",
      "Collecting pytorch-ssim\r\n",
      "  Downloading pytorch_ssim-0.1.tar.gz (1.4 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\r\n",
      "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (1.15.2)\r\n",
      "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (3.4.2)\r\n",
      "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (11.1.0)\r\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.37.0)\r\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.3.30)\r\n",
      "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (25.0)\r\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.4)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2.4.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->pandas) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->pandas) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->pandas) (2024.2.0)\r\n",
      "Building wheels for collected packages: pytorch-ssim\r\n",
      "  Building wheel for pytorch-ssim (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for pytorch-ssim: filename=pytorch_ssim-0.1-py3-none-any.whl size=2006 sha256=79ad9781ab7ff6ecffa5b8026904d3becdccaad60c4b9caa08881e8d8bbbf95e\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/58/68/a2/68a41e8268a076c128bbc3988d243187fa4681828e648bf1ca\r\n",
      "Successfully built pytorch-ssim\r\n",
      "Installing collected packages: pytorch-ssim\r\n",
      "Successfully installed pytorch-ssim-0.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas openpyxl scikit-image pytorch-ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f96614bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T10:40:09.031396Z",
     "iopub.status.busy": "2025-05-14T10:40:09.031118Z",
     "iopub.status.idle": "2025-05-14T10:40:10.549006Z",
     "shell.execute_reply": "2025-05-14T10:40:10.548173Z"
    },
    "id": "hA10DjyPxPY1",
    "papermill": {
     "duration": 1.523464,
     "end_time": "2025-05-14T10:40:10.550205",
     "exception": false,
     "start_time": "2025-05-14T10:40:09.026741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/deblur/pytorch/default/1/Generator.pth\n",
      "/kaggle/input/low-light/pytorch/default/1/model_epoch_100.pth\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ca30498",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T10:40:10.558869Z",
     "iopub.status.busy": "2025-05-14T10:40:10.558480Z",
     "iopub.status.idle": "2025-05-14T10:40:10.564694Z",
     "shell.execute_reply": "2025-05-14T10:40:10.564029Z"
    },
    "papermill": {
     "duration": 0.01182,
     "end_time": "2025-05-14T10:40:10.565851",
     "exception": false,
     "start_time": "2025-05-14T10:40:10.554031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/deblur/pytorch/default/1/Generator.pth\n",
      "/kaggle/input/low-light/pytorch/default/1/model_epoch_100.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4da89e63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T10:40:10.574053Z",
     "iopub.status.busy": "2025-05-14T10:40:10.573698Z",
     "iopub.status.idle": "2025-05-14T10:40:15.407712Z",
     "shell.execute_reply": "2025-05-14T10:40:15.406895Z"
    },
    "id": "dRYutNjvvteZ",
    "papermill": {
     "duration": 4.839654,
     "end_time": "2025-05-14T10:40:15.409139",
     "exception": false,
     "start_time": "2025-05-14T10:40:10.569485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pytorch_ssim\n",
    "import torch.nn as nn\n",
    "class CGSformerLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.7, beta=0.3):\n",
    "        super(CGSformerLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.ssim = pytorch_ssim.SSIM(window_size=11)\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        mse_loss = self.mse(output, target)\n",
    "        ssim_loss = 1 - self.ssim(output, target)\n",
    "        return self.alpha * mse_loss + self.beta * ssim_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "112a4dbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T10:40:15.417964Z",
     "iopub.status.busy": "2025-05-14T10:40:15.417606Z",
     "iopub.status.idle": "2025-05-14T10:40:20.516057Z",
     "shell.execute_reply": "2025-05-14T10:40:20.515480Z"
    },
    "id": "xdTiCVC8vwxj",
    "papermill": {
     "duration": 5.104312,
     "end_time": "2025-05-14T10:40:20.517418",
     "exception": false,
     "start_time": "2025-05-14T10:40:15.413106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "\n",
    "class LOLDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None,patch_size=128):\n",
    "        self.low_light_dir = os.path.join(root_dir, 'low')\n",
    "        self.high_light_dir = os.path.join(root_dir, 'high')\n",
    "        self.low_light_images = sorted(os.listdir(self.low_light_dir))\n",
    "        self.high_light_images = sorted(os.listdir(self.high_light_dir))\n",
    "        self.transform = transform\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.low_light_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        low_image_path = os.path.join(self.low_light_dir, self.low_light_images[idx])\n",
    "        high_image_path = os.path.join(self.high_light_dir, self.high_light_images[idx])\n",
    "\n",
    "        low_img = Image.open(low_image_path).convert('RGB')\n",
    "        high_img = Image.open(high_image_path).convert('RGB')\n",
    "\n",
    "        # Random crop\n",
    "        i, j, h, w = transforms.RandomCrop.get_params(low_img, output_size=(self.patch_size, self.patch_size))\n",
    "        low_img = transforms.functional.crop(low_img, i, j, h, w)\n",
    "        high_img = transforms.functional.crop(high_img, i, j, h, w)\n",
    "\n",
    "        # Random flip\n",
    "        if random.random() > 0.5:\n",
    "            low_img = transforms.functional.hflip(low_img)\n",
    "            high_img = transforms.functional.hflip(high_img)\n",
    "        if random.random() > 0.5:\n",
    "            low_img = transforms.functional.vflip(low_img)\n",
    "            high_img = transforms.functional.vflip(high_img)\n",
    "\n",
    "        if self.transform:\n",
    "            low_img = self.transform(low_img)\n",
    "            high_img = self.transform(high_img)\n",
    "\n",
    "        return low_img, high_img\n",
    "\n",
    "# Example transforms\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952c8af7",
   "metadata": {
    "id": "Elx3A-zXvy5F",
    "papermill": {
     "duration": 0.004157,
     "end_time": "2025-05-14T10:40:20.525533",
     "exception": false,
     "start_time": "2025-05-14T10:40:20.521376",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Sparse Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54687a37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T10:40:20.534120Z",
     "iopub.status.busy": "2025-05-14T10:40:20.533764Z",
     "iopub.status.idle": "2025-05-14T10:40:20.550106Z",
     "shell.execute_reply": "2025-05-14T10:40:20.549528Z"
    },
    "id": "tyfunvyov1hM",
    "papermill": {
     "duration": 0.021933,
     "end_time": "2025-05-14T10:40:20.551204",
     "exception": false,
     "start_time": "2025-05-14T10:40:20.529271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CFS(nn.Module): #Cross Feature Scrambling\n",
    "    def __init__(self, channels, threshold=0.5):\n",
    "        super(CFS, self).__init__()\n",
    "        self.threshold = threshold\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.gn = nn.GroupNorm(1, channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_ln = self.gn(x)\n",
    "        var = torch.var(x_ln, dim=[2,3], keepdim=True)\n",
    "        importance = var / (torch.sum(var, dim=1, keepdim=True) + 1e-6)\n",
    "        importance = self.sigmoid(importance)\n",
    "        mask_info = (importance > self.threshold).float()\n",
    "        mask_noninfo = (importance <= self.threshold).float()\n",
    "\n",
    "        x_info = mask_info * x_ln\n",
    "        x_noninfo = mask_noninfo * x_ln\n",
    "\n",
    "        pooled = F.adaptive_avg_pool2d(x_info + x_noninfo, (1, 1))\n",
    "        beta = self.sigmoid(pooled)\n",
    "\n",
    "        out = beta * x_info + (1 - beta) * x_noninfo\n",
    "        return out\n",
    "\n",
    "class ASA(nn.Module): #Adaptive Shift Attention\n",
    "    def __init__(self, channels, topk_ratio=0.5):\n",
    "        super(ASA, self).__init__()\n",
    "        self.topk_ratio = topk_ratio\n",
    "        self.query_conv = nn.Conv2d(channels, channels, 1)\n",
    "        self.key_conv = nn.Conv2d(channels, channels, 1)\n",
    "        self.value_conv = nn.Conv2d(channels, channels, 1)\n",
    "        self.scale = channels ** -0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.query_conv(x).flatten(2).transpose(1, 2)\n",
    "        k = self.key_conv(x).flatten(2).transpose(1, 2)\n",
    "        v = self.value_conv(x).flatten(2).transpose(1, 2)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        topk = int(attn.size(-1) * self.topk_ratio)\n",
    "        topk_values, _ = torch.topk(attn, k=topk, dim=-1)\n",
    "        threshold = topk_values[:, :, -1].unsqueeze(-1)\n",
    "        mask = attn >= threshold\n",
    "        attn = attn.masked_fill(~mask, float('-inf'))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "\n",
    "        out = attn @ v\n",
    "        out = out.transpose(1, 2).reshape(x.size())\n",
    "        return out\n",
    "\n",
    "class BGFF(nn.Module): #BIlateral Grid Feature Fusion\n",
    "    def __init__(self, channels):\n",
    "        super(BGFF, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 1)\n",
    "        self.conv_dw3x3 = nn.Conv2d(channels, channels, 3, padding=1, groups=channels)\n",
    "        self.conv_dw7x7 = nn.Conv2d(channels, channels, 7, padding=3, groups=channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 1)\n",
    "        self.swish = lambda x: x * torch.sigmoid(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        path1 = self.swish(self.conv_dw3x3(out))\n",
    "        path2 = self.swish(self.conv_dw7x7(out))\n",
    "        out = path1 * path2\n",
    "        out = self.conv2(out)\n",
    "        return out + x\n",
    "\n",
    "class CGSformerBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(CGSformerBlock, self).__init__()\n",
    "        self.cfs = CFS(channels)\n",
    "        self.asa = ASA(channels)\n",
    "        self.bgff = BGFF(channels)\n",
    "        self.norm1 = nn.LayerNorm([channels, 128, 128])\n",
    "        self.norm2 = nn.LayerNorm([channels, 128, 128])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_cfs = self.cfs(x)\n",
    "        x = self.asa(self.norm1(x_cfs)) + x\n",
    "        x = self.bgff(self.norm2(x)) + x\n",
    "        return x\n",
    "\n",
    "class SparseTransformer(nn.Module):\n",
    "    def __init__(self, channels=64):\n",
    "        super(SparseTransformer, self).__init__()\n",
    "        self.encoder = nn.Conv2d(3, channels, 3, padding=1)\n",
    "\n",
    "        self.block1 = CGSformerBlock(channels)\n",
    "        self.block2 = CGSformerBlock(channels)\n",
    "        self.block3 = CGSformerBlock(channels)\n",
    "        self.block4 = CGSformerBlock(channels)\n",
    "        self.block5 = CGSformerBlock(channels)\n",
    "        self.block6 = CGSformerBlock(channels)\n",
    "        self.block7 = CGSformerBlock(channels)\n",
    "\n",
    "        self.decoder = nn.Conv2d(channels, 3, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = self.block6(x)\n",
    "        x = self.block7(x)\n",
    "\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6385da35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T10:40:20.559484Z",
     "iopub.status.busy": "2025-05-14T10:40:20.559240Z",
     "iopub.status.idle": "2025-05-14T10:40:20.564823Z",
     "shell.execute_reply": "2025-05-14T10:40:20.564118Z"
    },
    "id": "z3AFNygNv4L4",
    "papermill": {
     "duration": 0.010965,
     "end_time": "2025-05-14T10:40:20.565934",
     "exception": false,
     "start_time": "2025-05-14T10:40:20.554969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def ssim(img1, img2, window_size=11):\n",
    "    channel = img1.shape[1]\n",
    "    window = torch.ones((channel, 1, window_size, window_size)).to(img1.device) / (window_size ** 2)\n",
    "\n",
    "    mu1 = F.conv2d(img1, window, padding=window_size // 2, groups=channel)\n",
    "    mu2 = F.conv2d(img2, window, padding=window_size // 2, groups=channel)\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1 ** 2, window, padding=window_size // 2, groups=channel) - mu1 ** 2\n",
    "    sigma2_sq = F.conv2d(img2 ** 2, window, padding=window_size // 2, groups=channel) - mu2 ** 2\n",
    "    sigma12 = F.conv2d(img1 * img2, window, padding=window_size // 2, groups=channel) - mu1 * mu2\n",
    "\n",
    "    C1, C2 = 0.01**2, 0.03**2  # Stability constants\n",
    "    ssim_map = ((2 * mu1 * mu2 + C1) * (2 * sigma12 + C2)) / ((mu1 ** 2 + mu2 ** 2 + C1) * (sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "    return ssim_map.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cdd9e8",
   "metadata": {
    "id": "AtMzrFU4wQC7",
    "papermill": {
     "duration": 0.00329,
     "end_time": "2025-05-14T10:40:20.572924",
     "exception": false,
     "start_time": "2025-05-14T10:40:20.569634",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "DGUNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "951c1f6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T10:40:20.580876Z",
     "iopub.status.busy": "2025-05-14T10:40:20.580648Z",
     "iopub.status.idle": "2025-05-14T10:40:20.620434Z",
     "shell.execute_reply": "2025-05-14T10:40:20.619874Z"
    },
    "id": "nzPZYDUSwR_y",
    "papermill": {
     "duration": 0.045126,
     "end_time": "2025-05-14T10:40:20.621407",
     "exception": false,
     "start_time": "2025-05-14T10:40:20.576281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pdb import set_trace as stx\n",
    "\n",
    "def conv(in_channels, out_channels, kernel_size, bias=False, stride=1):\n",
    "    return nn.Conv2d(\n",
    "        in_channels, out_channels, kernel_size,\n",
    "        padding=(kernel_size // 2), bias=bias, stride=stride)\n",
    "\n",
    "\n",
    "def conv_down(in_chn, out_chn, bias=False):\n",
    "    layer = nn.Conv2d(in_chn, out_chn, kernel_size=4, stride=2, padding=1, bias=bias)\n",
    "    return layer\n",
    "\n",
    "\n",
    "def default_conv(in_channels, out_channels, kernel_size,stride=1, bias=True):\n",
    "    return nn.Conv2d(\n",
    "        in_channels, out_channels, kernel_size,\n",
    "        padding=(kernel_size//2),stride=stride, bias=bias)\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, conv, n_feats, kernel_size,\n",
    "        bias=True, bn=False, act=nn.PReLU(), res_scale=1):\n",
    "\n",
    "        super(ResBlock, self).__init__()\n",
    "        m = []\n",
    "        for i in range(2):\n",
    "            if i == 0:\n",
    "                m.append(conv(n_feats, 64, kernel_size, bias=bias))\n",
    "            else:\n",
    "                m.append(conv(64, n_feats, kernel_size, bias=bias))\n",
    "            if bn:\n",
    "                m.append(nn.BatchNorm2d(n_feats))\n",
    "            if i == 0:\n",
    "                m.append(act)\n",
    "\n",
    "        self.body = nn.Sequential(*m)\n",
    "        self.res_scale = res_scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.body(x).mul(self.res_scale)\n",
    "        res += x\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "class CAB(nn.Module):\n",
    "    def __init__(self, n_feat, kernel_size, reduction, bias, act):\n",
    "        super(CAB, self).__init__()\n",
    "        modules_body = []\n",
    "        modules_body.append(conv(n_feat, n_feat, kernel_size, bias=bias))\n",
    "        modules_body.append(act)\n",
    "        modules_body.append(conv(n_feat, n_feat, kernel_size, bias=bias))\n",
    "\n",
    "        self.CA = CALayer(n_feat, reduction, bias=bias)\n",
    "        self.body = nn.Sequential(*modules_body)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.body(x)\n",
    "        res = self.CA(res)\n",
    "        res += x\n",
    "        return res\n",
    "\n",
    "\n",
    "class CALayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16, bias=False):\n",
    "        super(CALayer, self).__init__()\n",
    "        # global average pooling: feature --> point\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        # feature channel downscale and upscale --> channel weight\n",
    "        self.conv_du = nn.Sequential(\n",
    "            nn.Conv2d(channel, channel // reduction, 1, padding=0, bias=bias),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channel // reduction, channel, 1, padding=0, bias=bias),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.avg_pool(x)\n",
    "        y = self.conv_du(y)\n",
    "        return x * y\n",
    "\n",
    "\n",
    "class SAM(nn.Module):\n",
    "    def __init__(self, n_feat, kernel_size, bias):\n",
    "        super(SAM, self).__init__()\n",
    "        self.conv1 = conv(n_feat, n_feat, kernel_size, bias=bias)\n",
    "        self.conv2 = conv(n_feat, 3, kernel_size, bias=bias)\n",
    "\n",
    "    def forward(self, x, x_img):\n",
    "        x1 = self.conv1(x)\n",
    "        img = self.conv2(x) + x_img\n",
    "        x1 = x1 + x\n",
    "        return x1, img\n",
    "\n",
    "\n",
    "class mergeblock(nn.Module):\n",
    "    def __init__(self, n_feat, kernel_size, bias, subspace_dim=16):\n",
    "        super(mergeblock, self).__init__()\n",
    "        self.conv_block = conv(n_feat * 2, n_feat, kernel_size, bias=bias)\n",
    "        self.num_subspace = subspace_dim\n",
    "        self.subnet = conv(n_feat * 2, self.num_subspace, kernel_size, bias=bias)\n",
    "\n",
    "    def forward(self, x, bridge):\n",
    "        out = torch.cat([x, bridge], 1)\n",
    "        b_, c_, h_, w_ = bridge.shape\n",
    "        sub = self.subnet(out)\n",
    "        V_t = sub.view(b_, self.num_subspace, h_*w_)\n",
    "        V_t = V_t / (1e-6 + torch.abs(V_t).sum(axis=2, keepdims=True))\n",
    "        V = V_t.permute(0, 2, 1)\n",
    "        mat = torch.matmul(V_t, V)\n",
    "        mat_inv = torch.inverse(mat)\n",
    "        project_mat = torch.matmul(mat_inv, V_t)\n",
    "        bridge_ = bridge.view(b_, c_, h_*w_)\n",
    "        project_feature = torch.matmul(project_mat, bridge_.permute(0, 2, 1))\n",
    "        bridge = torch.matmul(V, project_feature).permute(0, 2, 1).view(b_, c_, h_, w_)\n",
    "        out = torch.cat([x, bridge], 1)\n",
    "        out = self.conv_block(out)\n",
    "        return out+x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_feat, kernel_size, reduction, act, bias, scale_unetfeats, csff,depth=5):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.body=nn.ModuleList()#[]\n",
    "        self.depth=depth\n",
    "        for i in range(depth-1):\n",
    "            self.body.append(UNetConvBlock(in_size=n_feat+scale_unetfeats*i, out_size=n_feat+scale_unetfeats*(i+1), downsample=True, relu_slope=0.2, use_csff=csff, use_HIN=True))\n",
    "        self.body.append(UNetConvBlock(in_size=n_feat+scale_unetfeats*(depth-1), out_size=n_feat+scale_unetfeats*(depth-1), downsample=False, relu_slope=0.2, use_csff=csff, use_HIN=True))\n",
    "\n",
    "    def forward(self, x, encoder_outs=None, decoder_outs=None):\n",
    "        res=[]\n",
    "        if encoder_outs is not None and decoder_outs is not None:\n",
    "            for i,down in enumerate(self.body):\n",
    "                if (i+1) < self.depth:\n",
    "                    x, x_up = down(x,encoder_outs[i],decoder_outs[-i-1])\n",
    "                    res.append(x_up)\n",
    "                else:\n",
    "                    x = down(x)\n",
    "        else:\n",
    "            for i,down in enumerate(self.body):\n",
    "                if (i+1) < self.depth:\n",
    "                    x, x_up = down(x)\n",
    "                    res.append(x_up)\n",
    "                else:\n",
    "                    x = down(x)\n",
    "        return res,x\n",
    "\n",
    "\n",
    "class UNetConvBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, downsample, relu_slope, use_csff=False, use_HIN=False):\n",
    "        super(UNetConvBlock, self).__init__()\n",
    "        self.downsample = downsample\n",
    "        self.identity = nn.Conv2d(in_size, out_size, 1, 1, 0)\n",
    "        self.use_csff = use_csff\n",
    "\n",
    "        self.conv_1 = nn.Conv2d(in_size, out_size, kernel_size=3, padding=1, bias=True)\n",
    "        self.relu_1 = nn.LeakyReLU(relu_slope, inplace=False)\n",
    "        self.conv_2 = nn.Conv2d(out_size, out_size, kernel_size=3, padding=1, bias=True)\n",
    "        self.relu_2 = nn.LeakyReLU(relu_slope, inplace=False)\n",
    "\n",
    "        if downsample and use_csff:\n",
    "            self.csff_enc = nn.Conv2d(out_size, out_size, 3, 1, 1)\n",
    "            self.csff_dec = nn.Conv2d(in_size, out_size, 3, 1, 1)\n",
    "            self.phi = nn.Conv2d(out_size, out_size, 3, 1, 1)\n",
    "            self.gamma = nn.Conv2d(out_size, out_size, 3, 1, 1)\n",
    "\n",
    "        if use_HIN:\n",
    "            self.norm = nn.InstanceNorm2d(out_size//2, affine=True)\n",
    "        self.use_HIN = use_HIN\n",
    "\n",
    "        if downsample:\n",
    "            self.downsample = conv_down(out_size, out_size, bias=False)\n",
    "\n",
    "    def forward(self, x, enc=None, dec=None):\n",
    "        out = self.conv_1(x)\n",
    "\n",
    "        if self.use_HIN:\n",
    "            out_1, out_2 = torch.chunk(out, 2, dim=1)\n",
    "            out = torch.cat([self.norm(out_1), out_2], dim=1)\n",
    "        out = self.relu_1(out)\n",
    "        out = self.relu_2(self.conv_2(out))\n",
    "\n",
    "        out += self.identity(x)\n",
    "        if enc is not None and dec is not None:\n",
    "            assert self.use_csff\n",
    "            skip_ = F.leaky_relu(self.csff_enc(enc) + self.csff_dec(dec), 0.1, inplace=True)\n",
    "            out = out*F.sigmoid(self.phi(skip_)) + self.gamma(skip_) + out\n",
    "        if self.downsample:\n",
    "            out_down = self.downsample(out)\n",
    "            return out_down, out\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "class UNetUpBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, relu_slope):\n",
    "        super(UNetUpBlock, self).__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2, bias=True)\n",
    "        self.conv_block = UNetConvBlock(out_size*2, out_size, False, relu_slope)\n",
    "\n",
    "    def forward(self, x, bridge):\n",
    "        up = self.up(x)\n",
    "        out = torch.cat([up, bridge], 1)\n",
    "        out = self.conv_block(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_feat, kernel_size, reduction, act, bias, scale_unetfeats,depth=5):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.body=nn.ModuleList()\n",
    "        self.skip_conv=nn.ModuleList()#[]\n",
    "        for i in range(depth-1):\n",
    "            self.body.append(UNetUpBlock(in_size=n_feat+scale_unetfeats*(depth-i-1), out_size=n_feat+scale_unetfeats*(depth-i-2), relu_slope=0.2))\n",
    "            self.skip_conv.append(nn.Conv2d(n_feat+scale_unetfeats*(depth-i-1), n_feat+scale_unetfeats*(depth-i-2), 3, 1, 1))\n",
    "\n",
    "    def forward(self, x, bridges):\n",
    "        res=[]\n",
    "        for i,up in enumerate(self.body):\n",
    "            x=up(x,self.skip_conv[i](bridges[-i-1]))\n",
    "            res.append(x)\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, in_channels, s_factor):\n",
    "        super(DownSample, self).__init__()\n",
    "        self.down = nn.Sequential(nn.Upsample(scale_factor=0.5, mode='bilinear', align_corners=False),\n",
    "                                  nn.Conv2d(in_channels, in_channels + s_factor, 1, stride=1, padding=0, bias=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.down(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UpSample, self).__init__()\n",
    "        self.up = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "                                nn.Conv2d(in_channels, out_channels, 1, stride=1, padding=0, bias=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Basic_block(nn.Module):\n",
    "    def __init__(self, in_c=3, out_c=3, n_feat=80, scale_unetfeats=48, scale_orsnetfeats=32, num_cab=8, kernel_size=3, reduction=4, bias=False):\n",
    "        super(Basic_block, self).__init__()\n",
    "        act = nn.PReLU()\n",
    "        self.phi_1 = ResBlock(default_conv,3,3)\n",
    "        self.phit_1 = ResBlock(default_conv,3,3)\n",
    "        self.shallow_feat2 = nn.Sequential(conv(3, n_feat, kernel_size, bias=bias),\n",
    "                                           CAB(n_feat, kernel_size, reduction, bias=bias, act=act))\n",
    "        self.stage2_encoder = Encoder(n_feat, kernel_size, reduction, act, bias, scale_unetfeats,depth=4, csff=True)\n",
    "        self.stage2_decoder = Decoder(n_feat, kernel_size, reduction, act, bias, scale_unetfeats,depth=4)\n",
    "        self.sam23 = SAM(n_feat, kernel_size=1, bias=bias)\n",
    "        self.r1 = nn.Parameter(torch.Tensor([0.5]))\n",
    "        self.concat12 = conv(n_feat * 2, n_feat, kernel_size, bias=bias)\n",
    "\n",
    "        self.merge12=mergeblock(n_feat,3,True)\n",
    "\n",
    "    def forward(self, img,stage1_img,feat1,res1,x2_samfeats):\n",
    "        phixsy_2 = self.phi_1(stage1_img) - img\n",
    "        x2_img = stage1_img - self.r1*self.phit_1(phixsy_2)\n",
    "        x2 = self.shallow_feat2(x2_img)\n",
    "        x2_cat = self.merge12(x2, x2_samfeats)\n",
    "        feat2,feat_fin2 = self.stage2_encoder(x2_cat, feat1, res1)\n",
    "        res2 = self.stage2_decoder(feat_fin2,feat2)\n",
    "        x3_samfeats, stage2_img = self.sam23(res2[-1], x2_img)\n",
    "        return x3_samfeats, stage2_img, feat2, res2\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_c=3, out_c=3, n_feat=80, scale_unetfeats=48, scale_orsnetfeats=32, num_cab=8, kernel_size=3,\n",
    "                 reduction=4, bias=False, depth=5):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        act = nn.PReLU()\n",
    "        self.depth=depth\n",
    "        self.basic=Basic_block(in_c, out_c, n_feat, scale_unetfeats, scale_orsnetfeats, num_cab, kernel_size, reduction, bias)\n",
    "        self.shallow_feat1 = nn.Sequential(conv(3, n_feat, kernel_size, bias=bias),\n",
    "                                           CAB(n_feat, kernel_size, reduction, bias=bias, act=act))\n",
    "        self.shallow_feat7 = nn.Sequential(conv(3, n_feat, kernel_size, bias=bias),\n",
    "                                           CAB(n_feat, kernel_size, reduction, bias=bias, act=act))\n",
    "\n",
    "        self.stage1_encoder = Encoder(n_feat, kernel_size, reduction, act, bias, scale_unetfeats,depth=4, csff=False)\n",
    "        self.stage1_decoder = Decoder(n_feat, kernel_size, reduction, act, bias, scale_unetfeats,depth=4)\n",
    "\n",
    "        self.sam12 = SAM(n_feat, kernel_size=1, bias=bias)\n",
    "\n",
    "        self.phi_0 = ResBlock(default_conv,3,3)\n",
    "        self.phit_0 = ResBlock(default_conv,3,3)\n",
    "        self.phi_6 = ResBlock(default_conv,3,3)\n",
    "        self.phit_6 = ResBlock(default_conv,3,3)\n",
    "        self.r0 = nn.Parameter(torch.Tensor([0.5]))\n",
    "        self.r6 = nn.Parameter(torch.Tensor([0.5]))\n",
    "\n",
    "        self.concat67 = conv(n_feat * 2, n_feat + scale_orsnetfeats, kernel_size, bias=bias)\n",
    "        self.tail = conv(n_feat + scale_orsnetfeats, 3, kernel_size, bias=bias)\n",
    "\n",
    "    def forward(self, img):\n",
    "        res=[]\n",
    "        phixsy_1 = self.phi_0(img) - img\n",
    "        x1_img = img - self.r0*self.phit_0(phixsy_1)\n",
    "        x1 = self.shallow_feat1(x1_img)\n",
    "        feat1,feat_fin1 = self.stage1_encoder(x1)\n",
    "        res1 = self.stage1_decoder(feat_fin1,feat1)\n",
    "        x2_samfeats, stage1_img = self.sam12(res1[-1], x1_img)\n",
    "        res.append(stage1_img)\n",
    "\n",
    "        for _ in range(self.depth):\n",
    "            x2_samfeats, stage1_img, feat1, res1 = self.basic(img,stage1_img,feat1,res1,x2_samfeats)\n",
    "            res.append(stage1_img)\n",
    "        phixsy_7 = self.phi_6(stage1_img) - img\n",
    "        x7_img = stage1_img - self.r6*self.phit_6(phixsy_7)\n",
    "        x7 = self.shallow_feat7(x7_img)\n",
    "        x7_cat = self.concat67(torch.cat([x7, x2_samfeats], 1))\n",
    "        stage7_img = self.tail(x7_cat)+ img\n",
    "        res.append(stage7_img)\n",
    "\n",
    "        return res[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b8526a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T10:40:20.630066Z",
     "iopub.status.busy": "2025-05-14T10:40:20.629632Z",
     "iopub.status.idle": "2025-05-14T10:40:20.639308Z",
     "shell.execute_reply": "2025-05-14T10:40:20.638752Z"
    },
    "id": "JumzQQ_Pv8Fn",
    "papermill": {
     "duration": 0.015029,
     "end_time": "2025-05-14T10:40:20.640320",
     "exception": false,
     "start_time": "2025-05-14T10:40:20.625291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def pad_to_multiple(img, patch_size=128):\n",
    "    \"\"\" Pad the image to multiple of patch size (no less than original size) \"\"\"\n",
    "    _, h, w = img.shape\n",
    "    pad_h = (patch_size - h % patch_size) % patch_size\n",
    "    pad_w = (patch_size - w % patch_size) % patch_size\n",
    "    img = F.pad(img, (0, pad_w, 0, pad_h), mode='reflect')\n",
    "    return img\n",
    "\n",
    "def split_patche(img, patch_size=128):\n",
    "    \"\"\" Split the image into non-overlapping patches \"\"\"\n",
    "    patches = []\n",
    "    coords = []\n",
    "    c, h, w = img.shape\n",
    "    for i in range(0, h, patch_size):\n",
    "        for j in range(0, w, patch_size):\n",
    "            patch = img[:, i:i+patch_size, j:j+patch_size]\n",
    "            patches.append(patch)\n",
    "            coords.append((i, j))\n",
    "    return patches, coords\n",
    "\n",
    "def merge_patche(patches, coords, image_shape, patch_size=128):\n",
    "    \"\"\" Merge patches back into full image \"\"\"\n",
    "    c, h, w = image_shape\n",
    "    merged = torch.zeros((c, h, w)).to(patches[0].device)\n",
    "    counter = torch.zeros((c, h, w)).to(patches[0].device)\n",
    "\n",
    "    for patch, (i, j) in zip(patches, coords):\n",
    "        merged[:, i:i+patch.shape[1], j:j+patch.shape[2]] += patch\n",
    "        counter[:, i:i+patch.shape[1], j:j+patch.shape[2]] += 1\n",
    "\n",
    "    counter[counter == 0] = 1\n",
    "    merged = merged / counter\n",
    "    return merged\n",
    "\n",
    "def enhance_image(model, img_path, save_path, device, patch_size=128):\n",
    "    \"\"\" Full enhancement pipeline \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Load image\n",
    "    img_tensor = TF.to_tensor(img_path).to(device)\n",
    "\n",
    "    c, h, w = img_tensor.shape\n",
    "\n",
    "    if h < patch_size or w < patch_size:\n",
    "        # If image is smaller in any dimension, pad to at least 128\n",
    "        img_tensor = pad_to_multiple(img_tensor, patch_size)\n",
    "        with torch.no_grad():\n",
    "            output = model(img_tensor.unsqueeze(0)).squeeze(0)\n",
    "        output = output[:, :h, :w]  # Crop back to original size\n",
    "    else:\n",
    "        # Normal size or large image\n",
    "        padded_img = pad_to_multiple(img_tensor, patch_size)\n",
    "        c_pad, h_pad, w_pad = padded_img.shape\n",
    "\n",
    "        patches, coords = split_patche(padded_img, patch_size)\n",
    "\n",
    "        enhanced_patches = []\n",
    "        with torch.no_grad():\n",
    "            for patch in patches:\n",
    "                out_patch = model(patch.unsqueeze(0)).squeeze(0)\n",
    "                enhanced_patches.append(out_patch)\n",
    "\n",
    "        merged = merge_patche(enhanced_patches, coords, (c_pad, h_pad, w_pad), patch_size)\n",
    "        output = merged[:, :h, :w]  # Remove padding to original size\n",
    "\n",
    "    output_img = TF.to_pil_image(torch.clamp(output, 0, 1).cpu())\n",
    "    output_img.save(save_path)\n",
    "    print(f\"Saved enhanced image at {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a7ed474",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T10:40:20.648787Z",
     "iopub.status.busy": "2025-05-14T10:40:20.648540Z",
     "iopub.status.idle": "2025-05-14T10:40:20.657814Z",
     "shell.execute_reply": "2025-05-14T10:40:20.657316Z"
    },
    "id": "Py3VUNzD0QN8",
    "papermill": {
     "duration": 0.014879,
     "end_time": "2025-05-14T10:40:20.658841",
     "exception": false,
     "start_time": "2025-05-14T10:40:20.643962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Load and initialize your trained model\n",
    "\n",
    "\n",
    "# Set patch size\n",
    "PATCH_SIZE = 128\n",
    "\n",
    "# Image transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "to_pil = transforms.ToPILImage()\n",
    "\n",
    "# Function to pad image\n",
    "def pad_image(image_tensor, patch_size):\n",
    "    _, h, w = image_tensor.shape\n",
    "    pad_h = (patch_size - h % patch_size) % patch_size\n",
    "    pad_w = (patch_size - w % patch_size) % patch_size\n",
    "    padded = F.pad(image_tensor, (0, pad_w, 0, pad_h), mode='reflect')\n",
    "    return padded, h, w\n",
    "\n",
    "# Function to draw patch grid\n",
    "def draw_patch_grid(image, patch_size):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    w, h = image.size\n",
    "    for x in range(0, w, patch_size):\n",
    "        draw.line([(x, 0), (x, h)], fill='red', width=1)\n",
    "    for y in range(0, h, patch_size):\n",
    "        draw.line([(0, y), (w, y)], fill='red', width=1)\n",
    "    return image\n",
    "\n",
    "# Function to split into patches\n",
    "def split_into_patches(image_tensor, patch_size):\n",
    "    _, h, w = image_tensor.shape\n",
    "    patches = []\n",
    "    for i in range(0, h, patch_size):\n",
    "        for j in range(0, w, patch_size):\n",
    "            patch = image_tensor[:, i:i+patch_size, j:j+patch_size]\n",
    "            patches.append((patch, i, j))\n",
    "    return patches, h, w\n",
    "\n",
    "# Function to merge patches\n",
    "def merge_patches(patches, full_h, full_w):\n",
    "    output = torch.zeros(3, full_h, full_w)\n",
    "    for patch_tensor, i, j in patches:\n",
    "        output[:, i:i+PATCH_SIZE, j:j+PATCH_SIZE] = patch_tensor\n",
    "    return output\n",
    "\n",
    "# Main inference function\n",
    "def process_image(image,device):\n",
    "    image = image.convert('RGB')\n",
    "    image_tensor = transform(image).to(device)\n",
    "    padded_image, orig_h, orig_w = pad_image(image_tensor, PATCH_SIZE)\n",
    "    patches, padded_h, padded_w = split_into_patches(padded_image, PATCH_SIZE)\n",
    "    processed_patches=[]\n",
    "    for patch, i, j in patches:\n",
    "        with torch.no_grad():\n",
    "            input_patch = patch.unsqueeze(0)  # Add batch dimension\n",
    "            output_patch = model(input_patch)\n",
    "            if isinstance(output_patch, (list, tuple)):\n",
    "                output_patch = output_patch[0]\n",
    "            processed_patches.append((output_patch.squeeze(0), i, j))\n",
    "\n",
    "\n",
    "    merged = merge_patches(processed_patches, padded_h, padded_w)\n",
    "    final = merged[:, :orig_h, :orig_w]  # Remove padding\n",
    "    output_img = to_pil(final.clamp(0, 1))\n",
    "\n",
    "    # Draw grid\n",
    "    output_img = draw_patch_grid(output_img, PATCH_SIZE)\n",
    "    return output_img\n",
    "\n",
    "# Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80835f29",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-05-14T10:40:20.666737Z",
     "iopub.status.busy": "2025-05-14T10:40:20.666534Z",
     "iopub.status.idle": "2025-05-14T10:40:30.006451Z",
     "shell.execute_reply": "2025-05-14T10:40:30.005622Z"
    },
    "papermill": {
     "duration": 9.345559,
     "end_time": "2025-05-14T10:40:30.008019",
     "exception": false,
     "start_time": "2025-05-14T10:40:20.662460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\r\n",
      "  Downloading gradio-5.29.0-py3-none-any.whl.metadata (16 kB)\r\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\r\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (22.1.0)\r\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\r\n",
      "Collecting fastapi<1.0,>=0.115.2 (from gradio)\r\n",
      "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\r\n",
      "Collecting ffmpy (from gradio)\r\n",
      "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Collecting gradio-client==1.10.0 (from gradio)\r\n",
      "  Downloading gradio_client-1.10.0-py3-none-any.whl.metadata (7.1 kB)\r\n",
      "Collecting groovy~=0.1 (from gradio)\r\n",
      "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\r\n",
      "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.31.1)\r\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\r\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\r\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (25.0)\r\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.3)\r\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\r\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\r\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\r\n",
      "Collecting python-multipart>=0.0.18 (from gradio)\r\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\r\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\r\n",
      "Collecting ruff>=0.9.3 (from gradio)\r\n",
      "  Downloading ruff-0.11.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\r\n",
      "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\r\n",
      "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\r\n",
      "Collecting semantic-version~=2.0 (from gradio)\r\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\r\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\r\n",
      "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\r\n",
      "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\r\n",
      "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\r\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\r\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\r\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\r\n",
      "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (2025.3.2)\r\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (15.0.1)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\r\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\r\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\r\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\r\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\r\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\r\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\r\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (14.0.0)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\r\n",
      "Downloading gradio-5.29.0-py3-none-any.whl (54.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading gradio_client-1.10.0-py3-none-any.whl (322 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.9/322.9 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\r\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\r\n",
      "Downloading ruff-0.11.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\r\n",
      "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\r\n",
      "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\r\n",
      "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\r\n",
      "Installing collected packages: uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, starlette, safehttpx, gradio-client, fastapi, gradio\r\n",
      "Successfully installed fastapi-0.115.12 ffmpy-0.5.0 gradio-5.29.0 gradio-client-1.10.0 groovy-0.1.2 python-multipart-0.0.20 ruff-0.11.9 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio opencv-python numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba26333b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T10:40:30.021574Z",
     "iopub.status.busy": "2025-05-14T10:40:30.020783Z",
     "iopub.status.idle": "2025-05-14T10:40:36.736970Z",
     "shell.execute_reply": "2025-05-14T10:40:36.736378Z"
    },
    "papermill": {
     "duration": 6.724037,
     "end_time": "2025-05-14T10:40:36.738080",
     "exception": false,
     "start_time": "2025-05-14T10:40:30.014043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "* Running on public URL: https://7518388dc2262d6fab.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://7518388dc2262d6fab.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Deblurring function using a simple sharpening kernel\n",
    "def deblur_image(image):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model= Generator()\n",
    "    model_path = '/kaggle/input/deblur/pytorch/default/1/Generator.pth'\n",
    "    checkpoint = torch.load(model_path, map_location='cpu')  # Ensure loaded to CPU\n",
    "    try:\n",
    "        model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    except:\n",
    "        state_dict = checkpoint[\"state_dict\"]\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in state_dict.items():\n",
    "            name = k[7:]  # remove `module.`\n",
    "            new_state_dict[name] = v\n",
    "        model.load_state_dict(new_state_dict)\n",
    "    model.eval()\n",
    "    output = process_image(image,device)\n",
    "    output.save('/kaggle/working/output_deblur.jpg')\n",
    "\n",
    "# Low light enhancement using histogram equalization\n",
    "def enhance_low_light(image):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = SparseTransformer().to(device)\n",
    "    model_path = '/kaggle/input/low-light/pytorch/default/1/model_epoch_100.pth'\n",
    "    model.load_state_dict(torch.load(model_path,map_location=device))  \n",
    "    enhance_image(model,image,\"/kaggle/working/output_low.png\",device)\n",
    "\n",
    "def run_image(image, enhancement_type):\n",
    "    if enhancement_type == \"Deblurring\":\n",
    "        deblur_image(image)\n",
    "        return Image.open('/kaggle/working/output_deblur.jpg')\n",
    "    elif enhancement_type == \"Low Light Enhancement\":\n",
    "        enhance_low_light(image)\n",
    "        return Image.open('/kaggle/working/output_low.png')\n",
    "    else:\n",
    "        return image \n",
    "# Gradio UI\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## 🖼️ Image Enhancement Tool\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            input_image = gr.Image(label=\"Upload Image\", type=\"pil\")\n",
    "            enhancement_option = gr.Radio([\"Deblurring\", \"Low Light Enhancement\"], label=\"Select Enhancement Type\")\n",
    "            submit_btn = gr.Button(\"Enhance Image\")\n",
    "        \n",
    "        with gr.Column():\n",
    "            submit_btn.click(fn=run_image, inputs=[input_image, enhancement_option], outputs = gr.Image(label=\"Enhanced Image\"))\n",
    "\n",
    "    \n",
    "\n",
    "# Launch the app\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f1c9de",
   "metadata": {
    "papermill": {
     "duration": 0.00558,
     "end_time": "2025-05-14T10:40:36.749852",
     "exception": false,
     "start_time": "2025-05-14T10:40:36.744272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 343133,
     "modelInstanceId": 322437,
     "sourceId": 391596,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 343292,
     "modelInstanceId": 322603,
     "sourceId": 391795,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 40.79542,
   "end_time": "2025-05-14T10:40:39.374467",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-14T10:39:58.579047",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
